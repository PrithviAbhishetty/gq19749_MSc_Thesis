{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UZcf0fwBBr_C"
      },
      "outputs": [],
      "source": [
        "# Installing some packages\n",
        "! pip install --upgrade jax jaxlib\n",
        "! pip install pmdarima\n",
        "! pip install statsmodels\n",
        "! pip install tensorflow\n",
        "! pip install keras\n",
        "! pip install scikit-optimize\n",
        "! pip install scikeras"
      ],
      "id": "UZcf0fwBBr_C"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mKcT9lMEmzsV"
      },
      "outputs": [],
      "source": [
        "# mount google drive in google colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "mKcT9lMEmzsV"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "de26d10b"
      },
      "source": [
        "# Loading in the packages"
      ],
      "id": "de26d10b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f4bf5a7c"
      },
      "outputs": [],
      "source": [
        "# Standard data wrangling and plotting packages\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.lines as mlines\n",
        "\n",
        "# Cleaning outputs\n",
        "import warnings\n",
        "\n",
        "# Stationarity analysis packages\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "from pmdarima.arima import ADFTest, ndiffs\n",
        "\n",
        "# Performance metrics packages\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from math import sqrt\n",
        "\n",
        "# ARIMA and cross-validation packages\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "from traitlets.traitlets import validate\n",
        "import concurrent.futures\n",
        "from concurrent.futures import ProcessPoolExecutor\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "import itertools\n",
        "from statsmodels.tools.sm_exceptions import ConvergenceWarning\n",
        "from pmdarima.arima import auto_arima\n",
        "\n",
        "# LSTM packages\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import Flatten\n",
        "\n",
        "from scikeras.wrappers import KerasRegressor\n",
        "from tensorflow.keras import Model, Input\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from skopt import BayesSearchCV\n",
        "import os\n",
        "import logging\n",
        "from tensorflow.keras import backend as K\n",
        "import tensorflow as tf\n",
        "\n",
        "# VAR/VECM packages\n",
        "import statsmodels.api as sm\n",
        "import statsmodels.tsa.vector_ar.vecm as vecm\n",
        "from statsmodels.tsa.api import VAR\n",
        "from statsmodels.tsa.vector_ar.var_model import VAR as var2"
      ],
      "id": "f4bf5a7c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5a61e41"
      },
      "source": [
        "# Loading and cleaning in the data"
      ],
      "id": "f5a61e41"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gdq4XbTkRcqk"
      },
      "source": [
        "### 0. Filter Datetime Range"
      ],
      "id": "Gdq4XbTkRcqk"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nvUyasO2Rb3U"
      },
      "outputs": [],
      "source": [
        "def filtertime(dataset, start, end):\n",
        "    dataset = dataset.copy()\n",
        "    dataset = dataset.loc[(dataset.index >= start) & (dataset.index <= end)]\n",
        "    return dataset"
      ],
      "id": "nvUyasO2Rb3U"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3e91e31a"
      },
      "source": [
        "### 1. SPY Dataset (Kaggle)"
      ],
      "id": "3e91e31a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "023fa937"
      },
      "outputs": [],
      "source": [
        "base_path = '/content/drive/My Drive/DSP/'\n",
        "\n",
        "SPY_1m = pd.read_csv(base_path + \"1_min_SPY_2008-2021.csv/1_min_SPY_2008-2021.csv\", index_col=False, parse_dates=['date'], infer_datetime_format=True)\n",
        "SPY_1m = SPY_1m[[\"date\", \"close\"]]\n",
        "SPY_1m.drop_duplicates(inplace=True)\n",
        "SPY_1m = SPY_1m.sort_values(by='date')\n",
        "SPY_1m.reset_index(drop=True, inplace=True)\n",
        "SPY_1m.index = SPY_1m.pop('date')\n",
        "SPY_1m = filtertime(SPY_1m, '2010-01-01 07:30:00', '2014-01-01 07:30:00')\n",
        "print(SPY_1m.isna().sum().sum())\n",
        "print(SPY_1m)\n",
        "SPY_1m.plot()\n",
        "plt.title('SPY Stock Price Time Series: 1min Frequency')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Close Price')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "id": "023fa937"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aj8XlrkSm_o7"
      },
      "source": [
        "### 2. Synthetic random walk time series"
      ],
      "id": "aj8XlrkSm_o7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "19jcOC0jnFZe"
      },
      "outputs": [],
      "source": [
        "# Seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Generate datetime index for one year with one-minute intervals\n",
        "start_date = pd.Timestamp(\"2023-01-01 00:00:00\")\n",
        "end_date = pd.Timestamp(\"2023-12-31 23:59:00\")\n",
        "date_range = pd.date_range(start=start_date, end=end_date, freq='T')\n",
        "\n",
        "# Generate random walk data spanning the length of date_range\n",
        "initial_value = 0\n",
        "n_points_new = len(date_range)\n",
        "steps_new = np.random.choice([-1, 1], size=n_points_new-1)  # Subtract 1 for the initial value\n",
        "time_series_new = np.concatenate(([initial_value], np.cumsum(steps_new)))\n",
        "\n",
        "# Create DataFrame with the datetime index\n",
        "rw_df = pd.DataFrame(time_series_new, columns=['Value'], index=date_range)\n",
        "rw_df.rename(columns={\"Value\":\"close\"}, inplace=True)\n",
        "print(rw_df.isna().sum().sum())\n",
        "print(rw_df)\n",
        "# Plot the generated random walk data\n",
        "# plt.figure(figsize=(12, 6))\n",
        "rw_df.plot()\n",
        "plt.title('Random Walk Time Series: 1min Frequency')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Close')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "id": "19jcOC0jnFZe"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuz4HDqx8UVg"
      },
      "source": [
        "### 3. Reyes financial dataset (Euro-dollar Exchange)"
      ],
      "id": "fuz4HDqx8UVg"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VMmUaMtn8jjR"
      },
      "outputs": [],
      "source": [
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"openpyxl.styles.stylesheet\")\n",
        "\n",
        "base_path = '/content/drive/My Drive/DSP/'\n",
        "\n",
        "Reyes_2005 = pd.DataFrame(pd.read_excel(base_path + \"Reyes/EURUSD_M1_2005.xlsx\", index_col=False, header=None))[[0,4]]\n",
        "Reyes_2006 = pd.DataFrame(pd.read_excel(base_path + \"Reyes/EURUSD_M1_2006.xlsx\", index_col=False, header=None))[[0,4]]\n",
        "Reyes_2007 = pd.DataFrame(pd.read_excel(base_path + \"Reyes/EURUSD_M1_2007.xlsx\", index_col=False, header=None))[[0,4]]\n",
        "Reyes_2008 = pd.DataFrame(pd.read_excel(base_path + \"Reyes/EURUSD_M1_2008.xlsx\", index_col=False, header=None))[[0,4]]\n",
        "Reyes_2009 = pd.DataFrame(pd.read_excel(base_path + \"Reyes/EURUSD_M1_2009.xlsx\", index_col=False, header=None))[[0,4]]\n",
        "Reyes_2010 = pd.DataFrame(pd.read_excel(base_path + \"Reyes/EURUSD_M1_2010.xlsx\", index_col=False, header=None))[[0,4]]\n",
        "Reyes_2011 = pd.DataFrame(pd.read_excel(base_path + \"Reyes/EURUSD_M1_2011.xlsx\", index_col=False, header=None))[[0,4]]\n",
        "Reyes_2012 = pd.DataFrame(pd.read_excel(base_path + \"Reyes/EURUSD_M1_2012.xlsx\", index_col=False, header=None))[[0,4]]\n",
        "Reyes_2013 = pd.DataFrame(pd.read_excel(base_path + \"Reyes/EURUSD_M1_2013.xlsx\", index_col=False, header=None))[[0,4]]\n",
        "Reyes_2014 = pd.DataFrame(pd.read_excel(base_path + \"Reyes/EURUSD_M1_2014.xlsx\", index_col=False, header=None))[[0,4]]\n",
        "Reyes_2015 = pd.DataFrame(pd.read_excel(base_path + \"Reyes/EURUSD_M1_2015.xlsx\", index_col=False, header=None))[[0,4]]"
      ],
      "id": "VMmUaMtn8jjR"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S6Fb1Ad31Cvq"
      },
      "outputs": [],
      "source": [
        "Reyes_1m = pd.concat([Reyes_2005,Reyes_2006,Reyes_2007,Reyes_2008,Reyes_2009,Reyes_2010,Reyes_2011,Reyes_2012,Reyes_2013,Reyes_2014,Reyes_2015], ignore_index=True)\n",
        "Reyes_1m.rename(columns={0:\"date\", 4:\"close\"}, inplace=True)\n",
        "Reyes_1m.drop_duplicates(inplace=True)\n",
        "Reyes_1m = Reyes_1m.sort_values(by=\"date\")\n",
        "Reyes_1m.reset_index(drop=True, inplace=True)\n",
        "Reyes_1m.index = Reyes_1m.pop(\"date\")\n",
        "Reyes_1m = filtertime(Reyes_1m, '2006-01-01 00:00:00', '2014-12-31 23:59:00')\n",
        "Reyes_5m = Reyes_1m.resample(\"5Min\").nearest()\n",
        "print(Reyes_1m.isna().sum().sum())\n",
        "print(Reyes_5m.isna().sum().sum())\n",
        "print(Reyes_5m)\n",
        "Reyes_5m.plot()\n",
        "plt.title('EUR-USD Forex Exchange Rate Time Series: 5min Frequency')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Close Rate')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "id": "S6Fb1Ad31Cvq"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEH9TNkwOP0b"
      },
      "source": [
        "### 4. Bitcoin Dataset (Kaggle)"
      ],
      "id": "qEH9TNkwOP0b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aolEf0JnOTWN"
      },
      "outputs": [],
      "source": [
        "base_path = '/content/drive/My Drive/DSP/'\n",
        "\n",
        "BTC_2017 = pd.DataFrame(pd.read_csv(base_path + 'BTC/BTC-2017min.csv', index_col=False, header=None, low_memory=False))[[1,6]]\n",
        "BTC_2018 = pd.DataFrame(pd.read_csv(base_path + 'BTC/BTC-2018min.csv', index_col=False, header=None, low_memory=False))[[1,6]]\n",
        "BTC_2019 = pd.DataFrame(pd.read_csv(base_path + 'BTC/BTC-2019min.csv', index_col=False, header=None, low_memory=False))[[1,6]]\n",
        "BTC_2020 = pd.DataFrame(pd.read_csv(base_path + 'BTC/BTC-2020min.csv', index_col=False, header=None, low_memory=False))[[1,6]]\n",
        "BTC_2021 = pd.DataFrame(pd.read_csv(base_path + 'BTC/BTC-2021min.csv', index_col=False, header=None, low_memory=False))[[1,6]]"
      ],
      "id": "aolEf0JnOTWN"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WOqnTxlE_XVk"
      },
      "outputs": [],
      "source": [
        "BTC_1m = pd.concat([BTC_2017, BTC_2018, BTC_2019, BTC_2020, BTC_2021], ignore_index=True)\n",
        "BTC_1m.rename(columns={1:\"date\", 6:\"close\"}, inplace=True)\n",
        "BTC_1m['close'] = pd.to_numeric(BTC_1m['close'], errors='coerce')\n",
        "BTC_1m.drop_duplicates(inplace=True)\n",
        "BTC_1m = BTC_1m.sort_values(by=\"date\")\n",
        "BTC_1m.reset_index(drop=True, inplace=True)\n",
        "BTC_1m = BTC_1m.iloc[:-1]\n",
        "BTC_1m['date'] = pd.to_datetime(BTC_1m['date'])\n",
        "BTC_1m.index = BTC_1m.pop(\"date\")\n",
        "BTC_1m = filtertime(BTC_1m, '2017-01-01 00:01:00', '2021-12-31 23:59:00')\n",
        "print(BTC_1m.isna().sum().sum())\n",
        "print(BTC_1m)\n",
        "BTC_1m.plot()\n",
        "plt.title('BTC Cryptocurrency Price Time Series: 1min Frequency')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Close Price')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "id": "WOqnTxlE_XVk"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "786c00HzuQQC"
      },
      "source": [
        "### Replace zeros with small value"
      ],
      "id": "786c00HzuQQC"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hRgwWjrNuWoe"
      },
      "outputs": [],
      "source": [
        "def replace(df):\n",
        "  df = df.copy()\n",
        "  # print((df == 0).sum().sum())\n",
        "  locations = df.where(df==0).stack().index.tolist()\n",
        "  # print(locations)\n",
        "  df = df.replace(0, 0.0001)\n",
        "  # print((df == 0).sum().sum())\n",
        "  return df"
      ],
      "id": "hRgwWjrNuWoe"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66fa763a"
      },
      "source": [
        "# Raw to symbolic boxplot time series"
      ],
      "id": "66fa763a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "69d8091a"
      },
      "outputs": [],
      "source": [
        "def rawtosymbolic(dataset, f, price):\n",
        "    dataset = dataset.copy()\n",
        "    # print(dataset.isna().sum().sum())\n",
        "    # print(dataset.dtypes)\n",
        "\n",
        "    # symbolic descriptive statistics\n",
        "    min_data = (dataset.groupby(pd.Grouper(freq=f, origin=\"start_day\")).min().dropna()).rename(columns={'close':'min'})\n",
        "    q25_data = dataset.groupby(pd.Grouper(freq=f, origin=\"start_day\")).quantile(q=0.25).dropna()\n",
        "    q50_data = dataset.groupby(pd.Grouper(freq=f, origin=\"start_day\")).quantile().dropna()\n",
        "    q75_data = dataset.groupby(pd.Grouper(freq=f, origin=\"start_day\")).quantile(q=0.75).dropna()\n",
        "    # min_data = (q25_data - (1.5*(q75_data - q25_data))).rename(columns={'close':'min'})\n",
        "    # max_data = (q75_data + (1.5*(q75_data - q25_data))).rename(columns={'close':'max'})\n",
        "    max_data = (dataset.groupby(pd.Grouper(freq=f, origin=\"start_day\")).max().dropna()).rename(columns={'close':'max'})\n",
        "\n",
        "    # max_data -= q75_data\n",
        "    q75_data -= q50_data\n",
        "    q50_data -= q25_data\n",
        "    # q25_data -= min_data\n",
        "\n",
        "    # print(q50_data.isna().sum().sum())\n",
        "    # print(q25_data.isna().sum().sum())\n",
        "    # print(q75_data.isna().sum().sum())\n",
        "    # print(len(q50_data))\n",
        "    # print(len(q25_data))\n",
        "    # print(len(q75_data))\n",
        "\n",
        "    # symbolic table\n",
        "    sym_table = pd.DataFrame(columns=[\"Q1\", \"Q2\", \"Q3\"])\n",
        "    # sym_table[\"min\"] = min_data[price]\n",
        "    sym_table[\"Q1\"] = q25_data[price]\n",
        "    sym_table[\"Q2\"] = q50_data[price]\n",
        "    sym_table[\"Q3\"] = q75_data[price]\n",
        "    # sym_table[\"max\"] = max_data[price]\n",
        "\n",
        "    # filter original dataframe to price column and single point average using mean\n",
        "    og_table = dataset[[price]].copy()\n",
        "    sp_table = dataset.groupby(pd.Grouper(freq=f, origin=\"start_day\")).mean().dropna()\n",
        "    sp_table = sp_table[[price]]\n",
        "\n",
        "    return sym_table, sp_table, og_table, min_data, max_data\n",
        "\n",
        "def math_coherence(temp):\n",
        "    temp = temp.copy()\n",
        "    df = pd.DataFrame(temp)\n",
        "    df.columns = ['Q1', 'Q2', 'Q3']\n",
        "    # df['Q1'] += df['min']\n",
        "    df['Q2'] += df['Q1']\n",
        "    df['Q3'] += df['Q2']\n",
        "    # df['max'] += df['Q3']\n",
        "    return df"
      ],
      "id": "69d8091a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ZbpL0GND22U"
      },
      "outputs": [],
      "source": [
        "# Synthetic data\n",
        "rw_sym_data, rw_sp_data, rw_raw_data, rw_min_data, rw_max_data = rawtosymbolic(rw_df, \"1D\", \"close\")\n",
        "rw_sym_data = replace(rw_sym_data)\n",
        "rw_sp_data = replace(rw_sp_data)\n",
        "rw_raw_data = replace(rw_raw_data)\n",
        "\n",
        "rw_sp_data.plot(label='close')\n",
        "plt.title('Random Walk, Freq:1day, Agg:mean')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Close Price')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "ax = math_coherence(rw_sym_data).plot()\n",
        "rw_min_data.plot(linestyle='--', ax=ax, linewidth=0.5, alpha=0.8)\n",
        "rw_max_data.plot(linestyle='--', ax=ax, linewidth=0.5, alpha=0.8)\n",
        "plt.title('Random Walk: Freq:1day, Agg: 3 boxplot variables')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Close Price')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "id": "-ZbpL0GND22U"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "69011096"
      },
      "outputs": [],
      "source": [
        "# S&P 500 dataset (Stock)\n",
        "spy_sym_data, spy_sp_data, spy_raw_data, spy_min_data, spy_max_data = rawtosymbolic(SPY_1m, \"1D\", \"close\")\n",
        "spy_sym_data = replace(spy_sym_data)\n",
        "spy_sp_data = replace(spy_sp_data)\n",
        "spy_raw_data = replace(spy_raw_data)\n",
        "\n",
        "spy_sp_data.plot()\n",
        "plt.title('SPY, Freq:1day, Agg:mean')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Close Price')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "ax = filtertime(math_coherence(spy_sym_data), '2011-01-01', '2011-02-01').plot()\n",
        "filtertime(spy_min_data, '2011-01-01', '2011-02-01').plot(linestyle='--', ax=ax, linewidth=0.5, alpha=0.8)\n",
        "filtertime(spy_max_data, '2011-01-01', '2011-02-01').plot(linestyle='--', ax=ax, linewidth=0.5, alpha=0.8)\n",
        "plt.title('SPY: Freq:1day, Agg: 3 boxplot variables')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Close Price')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "id": "69011096"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JPI4Rnf4FCfz"
      },
      "outputs": [],
      "source": [
        "# Eurodollar exchange dataset (Reyes et al.)\n",
        "reyes_sym_data, reyes_sp_data, reyes_raw_data, reyes_min_data, reyes_max_data = rawtosymbolic(Reyes_5m, \"1D\", \"close\")\n",
        "reyes_sym_data = replace(reyes_sym_data)\n",
        "reyes_sp_data = replace(reyes_sp_data)\n",
        "reyes_raw_data = replace(reyes_raw_data)\n",
        "\n",
        "reyes_sp_data.plot()\n",
        "plt.title('EUR-USD, Freq:1day, Agg:mean')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Close Rate')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "ax = filtertime(math_coherence(reyes_sym_data), '2005-01-01', '2015-01-01').plot()\n",
        "filtertime(reyes_min_data, '2005-01-01', '2015-01-01').plot(linestyle='--', ax=ax, linewidth=0.5, alpha=0.8)\n",
        "filtertime(reyes_max_data, '2005-01-01', '2015-01-01').plot(linestyle='--', ax=ax, linewidth=0.5, alpha=0.8)\n",
        "plt.title('EUR-USD: Freq:1day, Agg: 3 boxplot variables')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Close Rate')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "id": "JPI4Rnf4FCfz"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oLf8Kk8nX4By"
      },
      "outputs": [],
      "source": [
        "# Bitcoin Dataset (Cryptocurrency)\n",
        "BTC_sym_data, BTC_sp_data, BTC_raw_data, BTC_min_data, BTC_max_data = rawtosymbolic(BTC_1m, \"1D\", \"close\")\n",
        "BTC_sym_data = replace(BTC_sym_data)\n",
        "BTC_sp_data = replace(BTC_sp_data)\n",
        "BTC_raw_data = replace(BTC_raw_data)\n",
        "\n",
        "BTC_sp_data.plot()\n",
        "plt.title('BTC, Freq:1day, Agg:mean')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Close Price')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "ax = filtertime(math_coherence(BTC_sym_data), '2017-01-01','2022-01-01').plot()\n",
        "filtertime(BTC_min_data, '2017-01-01','2022-01-01').plot(linestyle='--', ax=ax, linewidth=0.5, alpha=0.8)\n",
        "filtertime(BTC_max_data, '2017-01-01','2022-01-01').plot(linestyle='--', ax=ax, linewidth=0.5, alpha=0.8)\n",
        "plt.title('BTC: Freq:1day, Agg: 3 boxplot variables')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Close Price')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "id": "oLf8Kk8nX4By"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5f6eb842"
      },
      "source": [
        "# Train-Test-Split the data"
      ],
      "id": "5f6eb842"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ea544deb"
      },
      "outputs": [],
      "source": [
        "# Manually defined function to prevent data leakage\n",
        "def traintestsplit(df, val_size, test_size):\n",
        "    df = df.copy()\n",
        "    test_size_ = int(len(df)*(1-test_size))\n",
        "    df_test = df[test_size_:]\n",
        "    if val_size == 0:\n",
        "      df_train = df[:test_size_]\n",
        "      return df_train, df_test\n",
        "    else:\n",
        "      val_size_ = int(len(df)*(1-test_size-val_size))\n",
        "      df_val = df[val_size_:test_size_]\n",
        "      df_train = df[:val_size_]\n",
        "      return df_train, df_val, df_test"
      ],
      "id": "ea544deb"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ea3e6aa1"
      },
      "source": [
        "# Stationarity analysis functions"
      ],
      "id": "ea3e6aa1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KJ353VHcdFsg"
      },
      "outputs": [],
      "source": [
        "# This function checks the stationarity of the input time series\n",
        "def check_stationarity(df):\n",
        "    df = df.copy()\n",
        "    # rolling statistics\n",
        "    rolling_mean = df.rolling(window = 12).mean()\n",
        "    rolling_std = df.rolling(window = 12).std()\n",
        "    # rolling statistics plot\n",
        "    plt.plot(df, color = 'blue', label = 'Original')\n",
        "    plt.plot(rolling_mean, color = 'red', label = 'Rolling Mean')\n",
        "    plt.plot(rolling_std, color = 'black', label = 'Rolling Std')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.legend(loc = 'best')\n",
        "    plt.title('Rolling Mean & Rolling Standard Deviation')\n",
        "    plt.show()\n",
        "    # acf plot\n",
        "    plot_acf(df)\n",
        "    plt.show()\n",
        "    # pacf_plot\n",
        "    plot_pacf(df)\n",
        "    plt.show()\n",
        "    # Dickey-Fuller statistical test\n",
        "    result = adfuller(df.dropna())\n",
        "    # print('ADF Statistic: {}'.format(result[0]))\n",
        "    print('p-value: {}'.format(result[1]))\n",
        "    # print('Critical Values:')\n",
        "    # for key, value in result[4].items():\n",
        "        # print('\\t{}: {}'.format(key, value))\n",
        "\n",
        "    # if p-value is over threshold (and ADF statistic is far from the critical values)\n",
        "    threshold = 0.05\n",
        "    if result[1] > threshold:\n",
        "        print('Time series is likely non-stationary.')\n",
        "    else:\n",
        "        print('Time series is likely stationary')\n",
        "\n",
        "    return"
      ],
      "id": "KJ353VHcdFsg"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e447ff87"
      },
      "outputs": [],
      "source": [
        "# This function explores different transformations of the dataset, and checks the stationarity for each transformation.\n",
        "\n",
        "def transformations(df, order):\n",
        "    df = df.copy()\n",
        "    # find logarithm of time series\n",
        "    df_log = np.log(df)\n",
        "    print(\"logarithm plot\")\n",
        "    plt.plot(df_log)\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.show()\n",
        "    check_stationarity(df_log)\n",
        "    # subtract rolling mean\n",
        "    rolling_mean = df.rolling(window=12).mean()\n",
        "    df_log_minus_mean = df_log - rolling_mean\n",
        "    df_log_minus_mean.dropna(inplace=True)\n",
        "    print(\"Rolling mean,std and ACF plots\")\n",
        "    check_stationarity(df_log_minus_mean)\n",
        "    # subtract rolling mean exponential decay\n",
        "    # rolling_mean_exp_decay = df.ewm(halflife=12, min_periods=0, adjust=True).mean()\n",
        "    # df_log_exp_decay = df_log - rolling_mean_exp_decay\n",
        "    # df_log_exp_decay.dropna(inplace=True)\n",
        "    # check_stationarity(df_log_exp_decay)\n",
        "    # find differenced time series\n",
        "    fig1, ax1 = plt.subplots()\n",
        "    ax1.set_title('{} Order Differencing'.format(order))\n",
        "    diff_df = df_log.copy()\n",
        "    for i in range(order):\n",
        "        diff_df = diff_df.diff()\n",
        "    ax1.plot(diff_df)\n",
        "    ax1.set_xlabel('Time')\n",
        "    ax1.set_ylabel('Differenced Value')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    fig2, (ax2, ax3) = plt.subplots(1, 2)\n",
        "    plot_pacf(diff_df.dropna(), ax=ax2)\n",
        "    ax2.set_title('PACF')\n",
        "    plot_acf(diff_df.dropna(), ax=ax3)\n",
        "    ax3.set_title('ACF')\n",
        "\n",
        "    # f = plt.figure()\n",
        "    # ax1 = f.add_subplot(121)\n",
        "    # ax1.set_title('{} Order Differencing'.format(order))\n",
        "    # diff_df = df.copy()\n",
        "    # for i in range(order):\n",
        "    #     diff_df = diff_df.diff()\n",
        "    # ax1.plot(diff_df)\n",
        "    # ax2 = f.add_subplot(122)\n",
        "    # plot_pacf(diff_df.dropna(), ax=ax2)\n",
        "    plt.show()\n",
        "    check_stationarity(diff_df)\n",
        "    return"
      ],
      "id": "e447ff87"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "166d4ee2"
      },
      "outputs": [],
      "source": [
        "# This function can be used to confirm the differencing term, d, using black-box statistical tests.\n",
        "\n",
        "def ADF_KPSS_PP(df):\n",
        "    df = df.copy()\n",
        "    # Test whether we should difference at the alpha=0.05 significance level\n",
        "    adf_test = ADFTest(alpha=0.05)\n",
        "    p_val, should_diff = adf_test.should_diff(df)\n",
        "\n",
        "    # Estimate the number of differences using an ADF, KPSS, and PP test:\n",
        "    n_adf = ndiffs(df, test='adf')\n",
        "    n_kpss = ndiffs(df, test='kpss')\n",
        "    n_pp = ndiffs(df, test='pp')\n",
        "\n",
        "    print(\"p_val: \", p_val)\n",
        "    print(\"should_diff: \", should_diff)\n",
        "    print(\"n_adf: \", n_adf)\n",
        "    print(\"n_kpss: \", n_kpss)\n",
        "    print(\"n_pp: \", n_pp)"
      ],
      "id": "166d4ee2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7QBQ0vCKiV4D"
      },
      "source": [
        "# Performance Metrics"
      ],
      "id": "7QBQ0vCKiV4D"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oxroGvtLia2x"
      },
      "outputs": [],
      "source": [
        "def sMAPE(y_true, y_pred):\n",
        "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
        "    # Compute the denominator, which is the sum of the absolute values of the actual and predicted values\n",
        "    denominator = np.abs(y_true) + np.abs(y_pred)\n",
        "    # Exclude cases where the denominator is zero to avoid division by zero\n",
        "    mask = denominator != 0\n",
        "    smape_value = (100/len(y_true)) * np.sum(np.abs(y_true[mask] - y_pred[mask]) / denominator[mask])\n",
        "    return round(smape_value, 4)\n",
        "\n",
        "def MMRE(y_true, y_pred):\n",
        "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
        "    # Exclude cases where y_true is zero to avoid division by zero\n",
        "    mask = y_true != 0\n",
        "    mmre_value = 100*np.mean(np.abs(y_true[mask] - y_pred[mask]) / y_true[mask])\n",
        "\n",
        "    # Rounding to 3 significant figures\n",
        "    return round(mmre_value, 4)\n",
        "\n",
        "def performance(observed, forecasts):\n",
        "    rmses = []\n",
        "    smapes = []\n",
        "    mmres = []\n",
        "\n",
        "    for col in observed.columns:\n",
        "        obs_col = observed[col]\n",
        "        fcast_col = forecasts[col]\n",
        "\n",
        "        rmse = round(sqrt(mean_squared_error(obs_col, fcast_col)),4)\n",
        "        smape = sMAPE(obs_col, fcast_col)\n",
        "        mmre = MMRE(obs_col, fcast_col)\n",
        "\n",
        "        print(f\"Performance for column {col}:\")\n",
        "        print(\"\\tRMSE:\", rmse)\n",
        "        print(\"\\tsMAPE:\", smape)\n",
        "        print(\"\\tMMRE:\", mmre)\n",
        "        print(\"--------\")\n",
        "\n",
        "        rmses.append(rmse)\n",
        "        smapes.append(smape)\n",
        "        mmres.append(mmre)\n",
        "\n",
        "    print(\"Overall Performance:\")\n",
        "    print(\"\\tAverage RMSE:\", round(np.mean(rmses),4))\n",
        "    print(\"\\tAverage sMAPE:\", round(np.mean(smapes),4))\n",
        "    print(\"\\tAverage MMRE:\", round(np.mean(mmres), 4))"
      ],
      "id": "oxroGvtLia2x"
    },
    {
      "cell_type": "code",
      "source": [
        "# forecast = pd.read_csv(base_path + '/save_forecasts/btc_arima_ms_forecasted.csv')\n",
        "# data = BTC_sym_data\n",
        "# train, test_df = traintestsplit(data, val_size=0, test_size=0.1)\n",
        "# test = math_coherence(test_df)\n",
        "\n",
        "# test['min'] = (test['Q1'] - (1.5*(test['Q3'] - test['Q1'])))\n",
        "# test['max'] = (test['Q3'] + (1.5*(test['Q3'] - test['Q1'])))\n",
        "# forecast['min'] = (forecast['Q1'] - (1.5*(forecast['Q3'] - forecast['Q1'])))\n",
        "# forecast['max'] = (forecast['Q3'] + (1.5*(forecast['Q3'] - forecast['Q1'])))\n",
        "\n",
        "# performance(test, forecast)"
      ],
      "metadata": {
        "id": "-w_mi_6vZGYg"
      },
      "id": "-w_mi_6vZGYg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jg7dfdnlyIwB"
      },
      "source": [
        "# Univariate Forecasting"
      ],
      "id": "jg7dfdnlyIwB"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "363624b8"
      },
      "source": [
        "### ARIMA"
      ],
      "id": "363624b8"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_jZ8AaXdAgr"
      },
      "source": [
        "##### Visualize the data"
      ],
      "id": "8_jZ8AaXdAgr"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G_xqm9XJa9Hq"
      },
      "outputs": [],
      "source": [
        "data = reyes_sym_data.copy()"
      ],
      "id": "G_xqm9XJa9Hq"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R0RYdbV09FoT"
      },
      "outputs": [],
      "source": [
        "train, test = traintestsplit(data['q25'], val_size=0, test_size=0.1)\n",
        "\n",
        "plt.plot(train.index, train)\n",
        "plt.plot(test.index, test)\n",
        "plt.legend(['Train', 'Test'])\n",
        "plt.ylabel('Close Price')\n",
        "plt.xlabel('Time')\n",
        "plt.xticks(rotation=45)\n",
        "plt.title('Data Visualisation')"
      ],
      "id": "R0RYdbV09FoT"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOXc-UiZbpBj"
      },
      "source": [
        "##### manual statistical (stationarity analysis functions)"
      ],
      "id": "pOXc-UiZbpBj"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GbsIzOIobe0I"
      },
      "outputs": [],
      "source": [
        "print(\"--CHECK STATIONARITY--\")\n",
        "check_stationarity(train)"
      ],
      "id": "GbsIzOIobe0I"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "anTsg8UlBejj"
      },
      "outputs": [],
      "source": [
        "print(\"--EXPLORE TRANSFORMATIONS--\")\n",
        "transformations(train, 1)"
      ],
      "id": "anTsg8UlBejj"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W3MTMMe3aMdN"
      },
      "outputs": [],
      "source": [
        "print(\"--CONFIRM DIFFERENCING ORDER--\")\n",
        "ADF_KPSS_PP(train)"
      ],
      "id": "W3MTMMe3aMdN"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DWkZbYfMhbmQ"
      },
      "outputs": [],
      "source": [
        "ms_order = [(0,1,1)]"
      ],
      "id": "DWkZbYfMhbmQ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGPC9pFIwLty"
      },
      "source": [
        "##### Model evaluation functions"
      ],
      "id": "iGPC9pFIwLty"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UlDlqFyimWFM"
      },
      "outputs": [],
      "source": [
        "def quick_test(train, test, param, f):\n",
        "    history = train.copy()\n",
        "    history.index = history.index.to_period(f)\n",
        "    arima = ARIMA(history, order=param).fit()\n",
        "    # residuals = arima.resid[1:]\n",
        "    # fig, ax = plt.subplots(1,2)\n",
        "    # # Check that the residuals look random and general.\n",
        "    # residuals.plot(title='Residuals', ax=ax[0])\n",
        "    # # Check that the density looks normally distributed, with a mean of around zero.\n",
        "    # residuals.plot(title='Density', kind='kde', ax=ax[1])\n",
        "    # plt.show()\n",
        "    # # Check that the lower lags barely show any significant spikes. This verifies that the residuals are close to white noise.\n",
        "    # acf_res = plot_acf(residuals)\n",
        "    # pacf_res = plot_pacf(residuals)\n",
        "\n",
        "    pred = arima.get_prediction(start=train.index[0], end=train.index[-1], dynamic=False).predicted_mean\n",
        "    pred.index = train.index\n",
        "    pred = pred.iloc[1:]\n",
        "\n",
        "    history = history.values.flatten().tolist()\n",
        "    forecasts = []\n",
        "    for t in range(0, len(test)-4, 5):\n",
        "        arima = ARIMA(history, order=param).fit()\n",
        "        forecasts.extend(arima.get_forecast(steps=5).predicted_mean)\n",
        "        for i in range(5):\n",
        "            history.append(test.iloc[t+i])\n",
        "    forecasts_df = pd.DataFrame(forecasts, index=test.index[:len(forecasts)])\n",
        "    # print(arima.summary())\n",
        "\n",
        "    return pred, forecasts_df"
      ],
      "id": "UlDlqFyimWFM"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oj_KYpaCEZ73"
      },
      "outputs": [],
      "source": [
        "# from pmdarima import ARIMA\n",
        "# from statsmodels.tsa.arima.model import ARIMA as smARIMA\n",
        "# import warnings\n",
        "# from statsmodels.tools.sm_exceptions import ConvergenceWarning\n",
        "# warnings.simplefilter('ignore', ConvergenceWarning)\n",
        "\n",
        "# def quick_test(train, test, order, f):\n",
        "\n",
        "#     history = train.copy()\n",
        "#     history.index = history.index.to_period(f)\n",
        "\n",
        "#     arima = ARIMA(order=order).fit(history)\n",
        "#     smarima = smARIMA(history, order=order).fit()\n",
        "\n",
        "#     # residuals = pd.Series(arima.resid(), index=history.index[1:])\n",
        "#     # fig, ax = plt.subplots(1,2)\n",
        "#     # residuals.plot(title='Residuals', ax=ax[0])\n",
        "#     # residuals.plot(title='Density', kind='kde', ax=ax[1])\n",
        "#     # plt.show()\n",
        "\n",
        "#     # plot_acf(residuals)\n",
        "#     # plot_pacf(residuals)\n",
        "\n",
        "#     pred = smarima.get_prediction(start=train.index[0], end=train.index[-1], dynamic=False).predicted_mean\n",
        "#     pred.index = train.index\n",
        "#     pred = pred.iloc[1:]\n",
        "\n",
        "#     forecasts = []\n",
        "#     for t in test.index:\n",
        "#         forecasts.append(arima.predict(n_periods=1)[0])\n",
        "#         arima.update(test.loc[[t]])\n",
        "\n",
        "#     forecasts_df = pd.DataFrame(forecasts, index=test.index)\n",
        "\n",
        "#     return pred, forecasts_df"
      ],
      "id": "Oj_KYpaCEZ73"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fOVk69FIpujY"
      },
      "outputs": [],
      "source": [
        "# pred, forecasts_df = quick_test(train, test['q25'], (3,1,0), 'D')\n",
        "# plt.figure(figsize=(10, 6))\n",
        "# plt.plot(train.index, train, label='Train')\n",
        "# plt.plot(test.index, test, label='Test')\n",
        "# plt.plot(forecasts_df.index, forecasts_df, label='Forecast')\n",
        "# plt.xticks(rotation=45)\n",
        "\n",
        "# plt.legend()\n",
        "# plt.grid(True)\n",
        "# plt.show()"
      ],
      "id": "fOVk69FIpujY"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qC_PQOf1S053"
      },
      "outputs": [],
      "source": [
        "def plot_forecasts(train, test, ms_pred_df, aa_pred_df, cv_pred_df, ms_forecast_df, aa_forecast_df, cv_forecast_df):\n",
        "\n",
        "    forecast_dfs = [ms_forecast_df, cv_forecast_df, aa_forecast_df]\n",
        "    forecast_labels = ['VS Testing Predictions', 'CV Testing Predictions', 'AA Testing Predictions']\n",
        "\n",
        "    if len(test.columns) != 1:\n",
        "        test['min'] = (test['Q1'] - (1.5*(test['Q3'] - test['Q1'])))\n",
        "        test['max'] = (test['Q3'] + (1.5*(test['Q3'] - test['Q1'])))\n",
        "        ms_forecast_df['min'] = (ms_forecast_df['Q1'] - (1.5*(ms_forecast_df['Q3'] - ms_forecast_df['Q1'])))\n",
        "        ms_forecast_df['max'] = (ms_forecast_df['Q3'] + (1.5*(ms_forecast_df['Q3'] - ms_forecast_df['Q1'])))\n",
        "        cv_forecast_df['min'] = (cv_forecast_df['Q1'] - (1.5*(cv_forecast_df['Q3'] - cv_forecast_df['Q1'])))\n",
        "        cv_forecast_df['max'] = (cv_forecast_df['Q3'] + (1.5*(cv_forecast_df['Q3'] - cv_forecast_df['Q1'])))\n",
        "        aa_forecast_df['min'] = (aa_forecast_df['Q1'] - (1.5*(aa_forecast_df['Q3'] - aa_forecast_df['Q1'])))\n",
        "        aa_forecast_df['max'] = (aa_forecast_df['Q3'] + (1.5*(aa_forecast_df['Q3'] - aa_forecast_df['Q1'])))\n",
        "\n",
        "        for forecast_df, forecast_label in zip(forecast_dfs, forecast_labels):\n",
        "            plt.figure(figsize=(10, 6))\n",
        "            plt.plot(test.index, test, label='Testing Observations', lw=2.0, color='orange')\n",
        "            plt.plot(forecast_df.index, forecast_df, label=forecast_label, color='green')\n",
        "\n",
        "            orange_line = mlines.Line2D([], [], color='orange', markersize=15, label='Testing Observations')\n",
        "            green_line = mlines.Line2D([], [], color='green', markersize=15, label=forecast_label)\n",
        "            plt.legend(handles=[orange_line, green_line])\n",
        "\n",
        "            plt.title(f'All symbolic rates ({forecast_label})')\n",
        "            plt.grid(True)\n",
        "            plt.show()\n",
        "\n",
        "    for i,col in enumerate(test.columns):\n",
        "        for forecast_df, forecast_label in zip(forecast_dfs, forecast_labels):\n",
        "            plt.figure(figsize=(10, 6))\n",
        "            plt.plot(test.index, test[col], label=f'{col} Testing Observations', lw=2.0, color='orange')\n",
        "            plt.plot(forecast_df.index, forecast_df[col], label=f'{col} {forecast_label}', color='green')\n",
        "            plt.legend()\n",
        "            plt.title(f'{col} rates ({forecast_label})')\n",
        "            plt.grid(True)\n",
        "            plt.show()\n",
        "\n",
        "    print(\"MS:\")\n",
        "    performance(test[:-4], ms_forecast_df)\n",
        "    print(\"AA:\")\n",
        "    performance(test[:-4], aa_forecast_df)\n",
        "    print(\"CC:\")\n",
        "    performance(test[:-4], cv_forecast_df)\n",
        "\n",
        "    # ms_forecast_df.to_csv(base_path + '/save_forecasts/spy_arima_sp_ms_forecasted.csv', index=False)\n",
        "    # aa_forecast_df.to_csv(base_path + '/save_forecasts/spy_arima_sp_aa_forecasted.csv', index=False)\n",
        "    # cv_forecast_df.to_csv(base_path + '/save_forecasts/spy_arima_sp_cv_forecasted.csv', index=False)"
      ],
      "id": "qC_PQOf1S053"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFPgkRFmvVa1"
      },
      "source": [
        "##### CV"
      ],
      "id": "kFPgkRFmvVa1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QoRUEun_vi5w"
      },
      "outputs": [],
      "source": [
        "# from traitlets.traitlets import validate\n",
        "# import concurrent.futures\n",
        "# from concurrent.futures import ProcessPoolExecutor\n",
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# from sklearn.metrics import mean_squared_error\n",
        "# from sklearn.model_selection import TimeSeriesSplit\n",
        "# import itertools\n",
        "# import warnings\n",
        "\n",
        "# def evaluate_arima_model(df, order, f):\n",
        "#         df = df.copy()\n",
        "#         error_scores = []\n",
        "#         tscv = TimeSeriesSplit(n_splits=3)\n",
        "#         for train_index, val_index in tscv.split(df):\n",
        "#             train, val = df.iloc[train_index], df.iloc[val_index]\n",
        "#             with warnings.catch_warnings():\n",
        "#                 warnings.filterwarnings(\"ignore\") # ignore all warnings\n",
        "#                 pred, forecasts = quick_test(train, val, order, f)\n",
        "#                 error = mean_squared_error(val, forecasts)\n",
        "#                 error_scores.append(error)\n",
        "#         return order, np.mean(error_scores)\n",
        "\n",
        "# def cv_approach(df, f):\n",
        "#     # Define the p, d and q parameters to take any value between 1 and 5\n",
        "#     p = d = q = range(0, 3)\n",
        "\n",
        "#     # Generate all different combinations of p, q and q triplets\n",
        "#     pdq = list(itertools.product(p, d, q))\n",
        "\n",
        "#     best_score, best_cfg = float(\"inf\"), None\n",
        "#     with concurrent.futures.ProcessPoolExecutor() as executor:\n",
        "#         futures = {executor.submit(evaluate_arima_model, df, order, f): order for order in pdq}\n",
        "#         for future in concurrent.futures.as_completed(futures):\n",
        "#             order, mse = future.result()\n",
        "#             if mse < best_score:\n",
        "#                 best_score, best_cfg = mse, order\n",
        "#             # print('ARIMA%s MSE=%.3f' % (order, mse))\n",
        "\n",
        "#     # print('Best ARIMA=%s MSE=%.3f' % (best_cfg, best_score))\n",
        "#     return best_cfg"
      ],
      "id": "QoRUEun_vi5w"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z_KzzS9ExlmP"
      },
      "outputs": [],
      "source": [
        "# out = cv_approach(train, f='D')\n",
        "# out"
      ],
      "id": "Z_KzzS9ExlmP"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vap9uJTswQyx"
      },
      "outputs": [],
      "source": [
        "warnings.simplefilter('ignore', ConvergenceWarning)\n",
        "\n",
        "def evaluate_arima_model(df, order, f):\n",
        "    df = df.copy()\n",
        "    error_scores = []\n",
        "    tscv = TimeSeriesSplit(n_splits=3)\n",
        "    for train_index, val_index in tscv.split(df):\n",
        "        train, val = df.iloc[train_index], df.iloc[val_index]\n",
        "        print('progress..')\n",
        "        with warnings.catch_warnings():\n",
        "            warnings.filterwarnings(\"ignore\")  # ignore all warnings\n",
        "            pred, forecasts = quick_test(train, val, order, f)\n",
        "            error = MMRE(val, forecasts)\n",
        "            error_scores.append(error)\n",
        "    return order, np.mean(error_scores)\n",
        "\n",
        "def stepwise_search(df, f):\n",
        "    p = d = q = [0, 1, 1]\n",
        "    best_score = float(\"inf\")\n",
        "    best_cfg = (0, 0, 0)  # initial configuration\n",
        "\n",
        "    improved = True  # flag to check if any improvements in the loop\n",
        "    while improved:\n",
        "        improved = False\n",
        "        neighbors = get_neighbors(best_cfg, p, d, q)\n",
        "\n",
        "        # Parallelize evaluations of neighbors\n",
        "        with concurrent.futures.ProcessPoolExecutor() as executor:\n",
        "            futures = {executor.submit(evaluate_arima_model, df, order, f): order for order in neighbors}\n",
        "            for future in concurrent.futures.as_completed(futures):\n",
        "                order, mse = future.result()\n",
        "                if mse < best_score:\n",
        "                    improved = True\n",
        "                    best_score = mse\n",
        "                    best_cfg = order\n",
        "\n",
        "    return best_cfg\n",
        "\n",
        "def get_neighbors(order, p_values, d_values, q_values):\n",
        "    neighbors = []\n",
        "\n",
        "    p, d, q = order\n",
        "    if p+1 in p_values:\n",
        "        neighbors.append((p+1, d, q))\n",
        "    if p-1 in p_values:\n",
        "        neighbors.append((p-1, d, q))\n",
        "    if d+1 in d_values:\n",
        "        neighbors.append((p, d+1, q))\n",
        "    if d-1 in d_values:\n",
        "        neighbors.append((p, d-1, q))\n",
        "    if q+1 in q_values:\n",
        "        neighbors.append((p, d, q+1))\n",
        "    if q-1 in q_values:\n",
        "        neighbors.append((p, d, q-1))\n",
        "\n",
        "    return neighbors"
      ],
      "id": "Vap9uJTswQyx"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yui4wrPgx5Bc"
      },
      "outputs": [],
      "source": [
        "# new = stepwise_search(train,'D')\n",
        "# new"
      ],
      "id": "yui4wrPgx5Bc"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkjal2nAbsP5"
      },
      "source": [
        "##### Auto Arima"
      ],
      "id": "wkjal2nAbsP5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rO6shXlMbvWi"
      },
      "outputs": [],
      "source": [
        "# from pmdarima.arima import auto_arima\n",
        "\n",
        "# def AutoArima(train, test, f):\n",
        "#     history = train.copy()\n",
        "#     history.index = history.index.to_period(f)\n",
        "#     forecast_auto = []\n",
        "#     best_params = []\n",
        "#     model3 = auto_arima(history, stepwise=False, seasonal=False, n_jobs=-1)\n",
        "#     pred_auto = model3.predict_in_sample()\n",
        "#     pred_auto.index = train.index\n",
        "#     pred_auto = pred_auto.iloc[1:]\n",
        "#     history = history.values.flatten().tolist()\n",
        "#     for t in range(len(test)):\n",
        "#         model3 = auto_arima(history, stepwise=False, seasonal=False, n_jobs=-1)\n",
        "#         forecast_auto.append(model3.predict(n_periods=1)[0])\n",
        "#         history.append(test.iloc[t,0])\n",
        "#         model3.summary()\n",
        "#         best_params.append(model3.order)\n",
        "#     forecast_auto_df = pd.DataFrame(forecast_auto, index=test.index)\n",
        "#     return best_params, pred_auto, forecast_auto_df"
      ],
      "id": "rO6shXlMbvWi"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PpK2gDvsYkyy"
      },
      "outputs": [],
      "source": [
        "warnings.simplefilter('ignore', ConvergenceWarning)\n",
        "\n",
        "def AutoArima(train, test, f):\n",
        "    history = train.copy()\n",
        "    history.index = history.index.to_period(f)\n",
        "    forecast_auto = []\n",
        "    model3 = auto_arima(history, stepwise=True, seasonal=False, n_jobs=1)\n",
        "    pred_auto = model3.predict_in_sample()\n",
        "    pred_auto.index = train.index\n",
        "    pred_auto = pred_auto.iloc[1:]\n",
        "    best_params = model3.order\n",
        "    # for t in test.index:\n",
        "    #     forecast_auto.append(model3.predict(n_periods=1)[0])\n",
        "    #     model3.update(test.loc[[t]])\n",
        "    # forecast_auto_df = pd.DataFrame(forecast_auto, index=test.index)\n",
        "    # return best_params, pred_auto, forecast_auto_df\n",
        "    for t in range(0, len(test)-4, 5):\n",
        "        forecast_auto.extend(model3.predict(n_periods=5))\n",
        "        model3.update(test.iloc[t:t+5])\n",
        "    forecast_auto_df = pd.DataFrame(forecast_auto, index=test.index[:len(forecast_auto)])\n",
        "    return best_params, pred_auto, forecast_auto_df"
      ],
      "id": "PpK2gDvsYkyy"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BrYGevlG_Ksw"
      },
      "outputs": [],
      "source": [
        "# best_params, pred_auto, forecast_auto = AutoArima(train, test, f='D')\n",
        "# best_params"
      ],
      "id": "BrYGevlG_Ksw"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qhAdI-1P_USV"
      },
      "outputs": [],
      "source": [
        "# plt.figure(figsize=(10, 6))\n",
        "# plt.plot(train.index, train, label='Train')\n",
        "# plt.plot(test.index, test, label='Test')\n",
        "# plt.plot(forecast_auto.index, forecast_auto, label='Forecast')\n",
        "# plt.xticks(rotation=45)\n",
        "\n",
        "# plt.legend()\n",
        "# plt.grid(True)\n",
        "# plt.show()"
      ],
      "id": "qhAdI-1P_USV"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vho43ntpIiCN"
      },
      "source": [
        "##### Arima pipeline"
      ],
      "id": "vho43ntpIiCN"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-5ToIQwdx9Nt"
      },
      "outputs": [],
      "source": [
        "def arima_pipeline(data, ms_order, cv_order, f):\n",
        "    train, test = traintestsplit(data, val_size=0, test_size=0.1)\n",
        "    ms_pred_, cv_pred_, aa_pred_, ms_forecast_, cv_forecast_, aa_forecast_ = pd.DataFrame(), pd.DataFrame(), pd.DataFrame(), pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n",
        "    stationary_train, stationary_test = train.copy(), test.copy()\n",
        "    stationary_train, stationary_test = np.log(stationary_train), np.log(stationary_test)\n",
        "    for i, col in enumerate(train.columns):\n",
        "        aa_order, aa_pred, aa_forecast = AutoArima(train[col], test[col], f)\n",
        "        print('aa params {}: {}'.format(col, aa_order))\n",
        "        aa_forecast_ = pd.concat([aa_forecast_,aa_forecast], axis=1)\n",
        "        aa_pred_ = pd.concat([aa_pred_,aa_pred], axis=1)\n",
        "\n",
        "        ms_pred, ms_forecast = quick_test(stationary_train[col], stationary_test[col], ms_order[i], f)\n",
        "        ms_pred_ = pd.concat([ms_pred_,ms_pred], axis=1)\n",
        "        ms_forecast_ = pd.concat([ms_forecast_,ms_forecast], axis=1)\n",
        "\n",
        "        # cv_order = stepwise_search(train[col], f)\n",
        "        print('cv params {}: {}'.format(col, cv_order[i]))\n",
        "        cv_pred, cv_forecast = quick_test(train[col], test[col], cv_order[i], f)\n",
        "        cv_pred_ = pd.concat([cv_pred_,cv_pred], axis=1)\n",
        "        cv_forecast_ = pd.concat([cv_forecast_,cv_forecast], axis=1)\n",
        "\n",
        "    aa_pred_.columns, aa_forecast_.columns = train.columns, test.columns\n",
        "    ms_pred_.columns, ms_forecast_.columns = train.columns, test.columns\n",
        "    cv_pred_.columns, cv_forecast_.columns = train.columns, test.columns\n",
        "\n",
        "    ms_pred_, ms_forecast_ = np.exp(ms_pred_), np.exp(ms_forecast_)\n",
        "\n",
        "    if len(train.columns) == 1:\n",
        "        plot_forecasts(train, test, ms_pred_, aa_pred_, cv_pred_, ms_forecast_, aa_forecast_, cv_forecast_)\n",
        "    else:\n",
        "        ms_pred_df = math_coherence(ms_pred_)\n",
        "        aa_pred_df = math_coherence(aa_pred_)\n",
        "        ms_forecast_df = math_coherence(ms_forecast_)\n",
        "        aa_forecast_df = math_coherence(aa_forecast_)\n",
        "        train_df = math_coherence(train)\n",
        "        test_df = math_coherence(test)\n",
        "        cv_pred_df = math_coherence(cv_pred_)\n",
        "        cv_forecast_df = math_coherence(cv_forecast_)\n",
        "        plot_forecasts(train_df, test_df, ms_pred_df, aa_pred_df, cv_pred_df, ms_forecast_df, aa_forecast_df, cv_forecast_df)"
      ],
      "id": "-5ToIQwdx9Nt"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VXqMuWg-8AYb"
      },
      "outputs": [],
      "source": [
        "# ms_order = [(1,1,1)]\n",
        "# cv_order = [(1,0,0)]\n",
        "ms_order = [(2,0,0),(1,0,1),(1,0,1)]\n",
        "cv_order = [(0,0,1),(0,0,0),(0,0,0)]"
      ],
      "id": "VXqMuWg-8AYb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dTg-vvpAOV8H"
      },
      "outputs": [],
      "source": [
        "arima_pipeline(reyes_sym_data.copy(), ms_order, cv_order, 'D')"
      ],
      "id": "dTg-vvpAOV8H"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a139ddd2"
      },
      "source": [
        "## LSTM"
      ],
      "id": "a139ddd2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### LSTM functions"
      ],
      "metadata": {
        "id": "D9CExOSc28Dr"
      },
      "id": "D9CExOSc28Dr"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eqvkXY5pilyV"
      },
      "outputs": [],
      "source": [
        "def df_to_windowed_df(df, window_size):\n",
        "    df = df.copy()\n",
        "    windowed_data = []\n",
        "    target_data = []\n",
        "    date_data = df.index\n",
        "    for i in range(window_size, len(df)):\n",
        "        window = df.iloc[i-window_size:i]\n",
        "        target = df.iloc[i].values\n",
        "        windowed_data.append(window)\n",
        "        target_data.append(target)\n",
        "\n",
        "    date_data, windowed_data, target_data = np.array(date_data), np.array(windowed_data), np.array(target_data)\n",
        "\n",
        "    return date_data[window_size:], windowed_data, target_data\n",
        "\n",
        "def plot_LSTM(dates_test, test_predictions, y_test, col):\n",
        "    # plt.plot(dates_train, train_predictions, color='magenta', label = 'Training Predictions')\n",
        "    # plt.plot(dates_train, y_train, color='red', label='Training Observations')\n",
        "    # plt.xticks(rotation=45)\n",
        "    # red_line = mlines.Line2D([], [], color='red', markersize=15, label='Training Observations')\n",
        "    # magenta_line = mlines.Line2D([], [], color='magenta', markersize=15, label='Training Predictions')\n",
        "    # plt.legend(handles=[red_line, magenta_line])\n",
        "    # plt.title(f'{col} price')\n",
        "    # plt.show()\n",
        "\n",
        "    # plt.plot(dates_val, val_predictions, color='peachpuff', label='Validation Predictions')\n",
        "    # plt.plot(dates_val, y_val, color='lightblue', label='Validation Observations')\n",
        "    # plt.xticks(rotation=45)\n",
        "    # lightblue_line = mlines.Line2D([], [], color='lightblue', markersize=15, label='Validation Observations')\n",
        "    # peachpuff_line = mlines.Line2D([], [], color='peachpuff', markersize=15, label='Validation Predictions')\n",
        "    # plt.legend(handles=[lightblue_line, peachpuff_line])\n",
        "    # plt.title(f'{col} price')\n",
        "    # plt.show()\n",
        "\n",
        "    plt.plot(dates_test, y_test, color='orange', label='Testing Observations')\n",
        "    plt.plot(dates_test, test_predictions, color='green', label='Testing Predictions')\n",
        "    plt.xticks(rotation=45)\n",
        "    orange_line = mlines.Line2D([], [], color='orange', markersize=15, label='Testing Observations')\n",
        "    green_line = mlines.Line2D([], [], color='green', markersize=15, label='Testing Predictions')\n",
        "    plt.legend(handles=[orange_line, green_line])\n",
        "    plt.title(f'{col} rates')\n",
        "    # plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    # plt.plot(dates_train, train_predictions, color='magenta', label = 'Training Predictions')\n",
        "    # plt.plot(dates_train, y_train, color='red', label='Training Observations')\n",
        "    # plt.plot(dates_val, val_predictions, color='peachpuff', label='Validation Predictions')\n",
        "    # plt.plot(dates_val, y_val, color='lightblue', label='Validation Observations')\n",
        "    # plt.plot(dates_test, test_predictions, color='orange', label='Testing Predictions')\n",
        "    # plt.plot(dates_test, y_test, color='blue', label='Testing Observations')\n",
        "\n",
        "    # Create a custom legend\n",
        "    # blue_line = mlines.Line2D([], [], color='blue', markersize=15, label='Testing Observations')\n",
        "    # orange_line = mlines.Line2D([], [], color='orange', markersize=15, label='Testing Predictions')\n",
        "    # lightblue_line = mlines.Line2D([], [], color='lightblue', markersize=15, label='Validation Observations')\n",
        "    # peachpuff_line = mlines.Line2D([], [], color='peachpuff', markersize=15, label='Validation Predictions')\n",
        "    # red_line = mlines.Line2D([], [], color='red', markersize=15, label='Training Observations')\n",
        "    # magenta_line = mlines.Line2D([], [], color='magenta', markersize=15, label='Training Predictions')\n",
        "    # plt.legend(handles=[blue_line, orange_line, lightblue_line, peachpuff_line, red_line, magenta_line])\n",
        "\n",
        "    # plt.title(f'{col} price')\n",
        "    # plt.show()"
      ],
      "id": "eqvkXY5pilyV"
    },
    {
      "cell_type": "code",
      "source": [
        "date, X, y = df_to_windowed_df(reyes_sym_data.copy(), 3)"
      ],
      "metadata": {
        "id": "tMqWnWbZWwU-"
      },
      "id": "tMqWnWbZWwU-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "date.shape"
      ],
      "metadata": {
        "id": "2-CrS6FOW7D6"
      },
      "id": "2-CrS6FOW7D6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I4YhGEOJB6Po"
      },
      "outputs": [],
      "source": [
        "# Suppress TensorFlow logs\n",
        "logging.getLogger('tensorflow').setLevel(logging.ERROR)\n",
        "\n",
        "# Suppress Python warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def LSTM_tuning(X_train, y_train, X_val, y_val):\n",
        "    np.random.seed(20)\n",
        "    tf.random.set_seed(20)\n",
        "    # Clear TensorFlow session\n",
        "    K.clear_session()\n",
        "    # Function to create model\n",
        "    def create_model(learn_rate=0.001, neurons=32, dropout_rate=0, activation='relu'):\n",
        "        # input layers\n",
        "        inputs = Input(shape=(3,1))\n",
        "        x = inputs\n",
        "        x = LSTM(units=neurons, activation=activation, return_sequences=True)(x)\n",
        "        # middle layer(s)\n",
        "        x = Dropout(dropout_rate)(x)\n",
        "        x = LSTM(units=neurons, activation=activation, return_sequences=False)(x)\n",
        "        x = Dense(units=neurons, activation=activation)(x)\n",
        "        x = Flatten()(x)\n",
        "        # last layer\n",
        "        outputs = Dense(1)(x)\n",
        "\n",
        "        model = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "        optimizer = Adam(learning_rate=learn_rate)\n",
        "        model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
        "\n",
        "        return model\n",
        "\n",
        "    # Wrap Keras model with KerasRegressor\n",
        "    model = KerasRegressor(build_fn=create_model, learn_rate=0.001, neurons=32, dropout_rate=0.0, activation='relu', verbose=0, epochs=50, batch_size=32)\n",
        "\n",
        "    # Define the parameter space\n",
        "    search_spaces = {\n",
        "        'learn_rate': (0.001, 0.01, 'log-uniform'),  # log-uniform distribution\n",
        "        'neurons': [32, 64, 128],  # list of choices\n",
        "        'dropout_rate': (0.0, 0.5, 'uniform'),  # uniform distribution\n",
        "        'batch_size': (32, 128, 'uniform'),  # uniform distribution\n",
        "    }\n",
        "    # Create TimeSeriesSplit object\n",
        "    tscv = TimeSeriesSplit(n_splits=3)\n",
        "\n",
        "    # Perform Bayesian Search Optimisation\n",
        "    bayes = BayesSearchCV(estimator=model, search_spaces=search_spaces, n_iter=10, cv=tscv, n_jobs=-1, random_state=20)\n",
        "    bayes_result = bayes.fit(X_train, y_train)\n",
        "\n",
        "    best_params = bayes_result.best_params_\n",
        "    # Build the model using best parameters\n",
        "    best_model = create_model(learn_rate=best_params['learn_rate'],\n",
        "                  neurons=best_params['neurons'],\n",
        "                  dropout_rate=best_params['dropout_rate'])\n",
        "\n",
        "    # Extract the best batch size from the best parameters\n",
        "    best_batch_size = best_params['batch_size']\n",
        "\n",
        "    # Define a ModelCheckpoint callback\n",
        "    cp = ModelCheckpoint('best_model_u', monitor='val_loss', verbose=0, save_best_only=True, mode='min')\n",
        "    # Fit the best model\n",
        "    best_model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=50, batch_size=best_params['batch_size'], callbacks=[cp], verbose=0)\n",
        "\n",
        "    # Load the best model\n",
        "    best_model.load_weights('best_model_u')\n",
        "\n",
        "    print(\"Best Hyperparameters:\")\n",
        "    for param, value in best_params.items():\n",
        "        print(f\"{param}: {value}\")\n",
        "\n",
        "    return best_model"
      ],
      "id": "I4YhGEOJB6Po"
    },
    {
      "cell_type": "code",
      "source": [
        "def iterative_forecasting(model, X, steps=5):\n",
        "    future_values = []\n",
        "    print(X.shape)\n",
        "    for i in range(0, len(X) - steps + 1, steps):\n",
        "        input_sequence = X[i:i+1, :].copy()\n",
        "        for _ in range(steps):\n",
        "            next_value = model.predict(input_sequence, verbose=0)\n",
        "            future_values.append(next_value.flatten())\n",
        "            input_sequence = np.roll(input_sequence, shift=-1, axis=1)\n",
        "            input_sequence[0, -1] = next_value\n",
        "\n",
        "    return np.array(future_values).reshape(-1,1)"
      ],
      "metadata": {
        "id": "nRQVxL11cs0G"
      },
      "id": "nRQVxL11cs0G",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xC6LP-rsfyD"
      },
      "source": [
        "##### step-by-step walkthrough"
      ],
      "id": "7xC6LP-rsfyD"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "acc35580"
      },
      "outputs": [],
      "source": [
        "data = reyes_sym_data.copy()\n",
        "data"
      ],
      "id": "acc35580"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VxOVcIAVELM8"
      },
      "outputs": [],
      "source": [
        "def df_to_windowed_df(df, window_size):\n",
        "    df = df.copy()\n",
        "    windowed_data = []\n",
        "    target_data = []\n",
        "    date_data = df.index\n",
        "    for i in range(window_size, len(df)):\n",
        "        window = df.iloc[i-window_size:i]\n",
        "        target = df.iloc[i]\n",
        "        windowed_data.append(window)\n",
        "        target_data.append(target)\n",
        "\n",
        "    date_data, windowed_data, target_data = np.array(date_data), np.array(windowed_data), np.array(target_data)\n",
        "\n",
        "    return date_data[window_size:], windowed_data, target_data"
      ],
      "id": "VxOVcIAVELM8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IH_FmMcfSDK3"
      },
      "outputs": [],
      "source": [
        "date, X, y = df_to_windowed_df(data, window_size=3)\n",
        "print(X.shape)\n",
        "print(y.shape)\n",
        "print(date.shape)"
      ],
      "id": "IH_FmMcfSDK3"
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "id": "4LGMenNAj4AI"
      },
      "id": "4LGMenNAj4AI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nhZat-blbxXl"
      },
      "outputs": [],
      "source": [
        "dates_train, dates_val, dates_test = traintestsplit(date, val_size=0.1, test_size=0.1)\n",
        "X_train, X_val, X_test = traintestsplit(X, val_size=0.1, test_size=0.1)\n",
        "y_train, y_val, y_test = traintestsplit(y, val_size=0.1, test_size=0.1)\n",
        "\n",
        "plt.plot(dates_train, y_train)\n",
        "plt.plot(dates_val, y_val)\n",
        "plt.plot(dates_test, y_test)\n",
        "\n",
        "plt.legend(['Train', 'Validation', 'Test'])\n",
        "plt.ylabel('Close Price')\n",
        "plt.xlabel('Time')\n",
        "plt.title('Stock: SPY (S&P 500), Range: 1yr, Frequency: 1day')"
      ],
      "id": "nhZat-blbxXl"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ukM71eNXzHX"
      },
      "outputs": [],
      "source": [
        "print(dates_train.shape)\n",
        "print(dates_val.shape)\n",
        "print(dates_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_val.shape)\n",
        "print(y_test.shape)\n",
        "print(X_train.shape)\n",
        "print(X_val.shape)\n",
        "print(X_test.shape)"
      ],
      "id": "2ukM71eNXzHX"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0YYYJ4vuepD3"
      },
      "outputs": [],
      "source": [
        "model = Sequential([layers.Input((3, 1)),\n",
        "                    layers.LSTM(64),\n",
        "                    layers.Dense(32, activation='relu'),\n",
        "                    layers.Dense(32, activation='relu'),\n",
        "                    layers.Dense(1)])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss='mse',\n",
        "              optimizer=Adam(learning_rate=0.001),\n",
        "              metrics=['mean_absolute_error'])\n",
        "\n",
        "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=50)"
      ],
      "id": "0YYYJ4vuepD3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EvFrqgnxk7Vf"
      },
      "outputs": [],
      "source": [
        "train_predictions = model.predict(X_train).flatten()\n",
        "\n",
        "plt.plot(dates_train, train_predictions)\n",
        "plt.plot(dates_train, y_train)\n",
        "plt.xticks(rotation=45)\n",
        "plt.legend(['Training Predictions', 'Training Observations'])"
      ],
      "id": "EvFrqgnxk7Vf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KZKbd-SBllx2"
      },
      "outputs": [],
      "source": [
        "val_predictions = model.predict(X_val).flatten()\n",
        "\n",
        "plt.plot(dates_val, val_predictions)\n",
        "plt.plot(dates_val, y_val)\n",
        "plt.xticks(rotation=45)\n",
        "plt.legend(['Validation Predictions', 'Validation Observations'])"
      ],
      "id": "KZKbd-SBllx2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gT9EVmAEl5uu"
      },
      "outputs": [],
      "source": [
        "test_predictions = model.predict(X_test).flatten()\n",
        "\n",
        "plt.plot(dates_test, test_predictions)\n",
        "plt.plot(dates_test, y_test)\n",
        "plt.xticks(rotation=45)\n",
        "plt.legend(['Testing Predictions', 'Testing Observations'])"
      ],
      "id": "gT9EVmAEl5uu"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MwJO963WmBO1"
      },
      "outputs": [],
      "source": [
        "plt.plot(dates_train, train_predictions)\n",
        "plt.plot(dates_train, y_train)\n",
        "plt.plot(dates_val, val_predictions)\n",
        "plt.plot(dates_val, y_val)\n",
        "plt.plot(dates_test, test_predictions)\n",
        "plt.plot(dates_test, y_test)\n",
        "plt.legend(['Training Predictions',\n",
        "            'Training Observations',\n",
        "            'Validation Predictions',\n",
        "            'Validation Observations',\n",
        "            'Testing Predictions',\n",
        "            'Testing Observations'])"
      ],
      "id": "MwJO963WmBO1"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7z5z2endaqbH"
      },
      "source": [
        "##### LSTM (univariate) pipeline"
      ],
      "id": "7z5z2endaqbH"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CCC1yTUe8jmK"
      },
      "outputs": [],
      "source": [
        "def LSTM_pipeline(df, window_size):\n",
        "    train_predictions_df, val_predictions_df, test_predictions_df = pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n",
        "    date, X, y = df_to_windowed_df(df, window_size)\n",
        "    dates_train, dates_val, dates_test = traintestsplit(date, val_size=0.1, test_size=0.1)\n",
        "    X_train, X_val, X_test = traintestsplit(X, val_size=0.1, test_size=0.1)\n",
        "    y_train, y_val, y_test = traintestsplit(y, val_size=0.1, test_size=0.1)\n",
        "\n",
        "    for i, col in enumerate(df.columns):\n",
        "        print('progress!')\n",
        "        model = LSTM_tuning(X_train[:,:,i], y_train[:,i], X_val[:,:,i], y_val[:,i])\n",
        "\n",
        "        train_predictions = iterative_forecasting(model, X_train[:,:,i])\n",
        "        train_predictions_t = pd.DataFrame(train_predictions)\n",
        "        train_predictions_df = pd.concat([train_predictions_df, train_predictions_t], axis=1)\n",
        "\n",
        "        val_predictions = iterative_forecasting(model, X_val[:,:,i])\n",
        "        val_predictions_t = pd.DataFrame(val_predictions)\n",
        "        val_predictions_df = pd.concat([val_predictions_df, val_predictions_t], axis=1)\n",
        "\n",
        "        test_predictions = iterative_forecasting(model, X_test[:,:,i])\n",
        "        test_predictions_t = pd.DataFrame(test_predictions)\n",
        "        test_predictions_df = pd.concat([test_predictions_df, test_predictions_t], axis=1)\n",
        "\n",
        "    if len(df.columns) == 1:\n",
        "        y_test_df, y_val_df, y_train_df = pd.DataFrame(y_test), pd.DataFrame(y_val), pd.DataFrame(y_train)\n",
        "        y_test_df.columns, y_val_df.columns, y_train_df.columns, train_predictions_df.columns, val_predictions_df.columns, test_predictions_df.columns, = df.columns,df.columns,df.columns,df.columns,df.columns,df.columns\n",
        "        performance(y_test_df, test_predictions_df)\n",
        "        plot_LSTM(dates_train, dates_val, dates_test, train_predictions_df, val_predictions_df, test_predictions_df, y_train_df, y_val_df, y_test_df, df.columns.copy().tolist()[0])\n",
        "        # test_predictions_df.to_csv(base_path + '/save_forecasts/reyes_sp_lstm_forecasted.csv', index=False)\n",
        "    else:\n",
        "        y_train_df = y_train\n",
        "        y_val_df = y_val\n",
        "        y_test_ = math_coherence(y_test)\n",
        "        full_train_predictions = train_predictions_df\n",
        "        full_val_predictions = val_predictions_df\n",
        "        print(test_predictions_df)\n",
        "        print(test_predictions_df.shape)\n",
        "        print(y_test_.shape)\n",
        "        full_test_predictions = math_coherence(test_predictions_df)\n",
        "        print(full_test_predictions)\n",
        "\n",
        "        truncation = len(y_test_) // 5 * 5\n",
        "        y_test_df = y_test_.iloc[:truncation,:]\n",
        "        dates_test_df = dates_test[:truncation]\n",
        "        print(y_test_df.shape)\n",
        "        print(dates_test_df.shape)\n",
        "\n",
        "        for true, forecast in zip([y_test_df],[full_test_predictions]):\n",
        "            true['min'] = (true['Q1'] - (1.5*(true['Q3'] - true['Q1'])))\n",
        "            true['max'] = (true['Q3'] + (1.5*(true['Q3'] - true['Q1'])))\n",
        "            forecast['min'] = (forecast['Q1'] - (1.5*(forecast['Q3'] - forecast['Q1'])))\n",
        "            forecast['max'] = (forecast['Q3'] + (1.5*(forecast['Q3'] - forecast['Q1'])))\n",
        "\n",
        "        performance(y_test_df, full_test_predictions)\n",
        "\n",
        "        for col in y_test_df.columns:\n",
        "            plot_LSTM(dates_test_df, full_test_predictions[col], y_test_df[col], col)\n",
        "\n",
        "        # plt.plot(dates_train, full_train_predictions, color='magenta', label = 'Training Predictions')\n",
        "        # plt.plot(dates_train, y_train_df, color='red', label='Training Observations')\n",
        "        # plt.plot(dates_val, full_val_predictions, color='peachpuff', label='Validation Predictions')\n",
        "        # plt.plot(dates_val, y_val_df, color='lightblue', label='Validation Observations')\n",
        "        plt.plot(dates_test_df, y_test_df, color='orange', label='Testing Observations')\n",
        "        plt.plot(dates_test_df, full_test_predictions, color='green', label='Testing Predictions')\n",
        "\n",
        "        # Create a custom legend\n",
        "        orange_line = mlines.Line2D([], [], color='orange', markersize=15, label='Testing Observations')\n",
        "        green_line = mlines.Line2D([], [], color='green', markersize=15, label='Testing Predictions')\n",
        "        # lightblue_line = mlines.Line2D([], [], color='lightblue', markersize=15, label='Validation Observations')\n",
        "        # peachpuff_line = mlines.Line2D([], [], color='peachpuff', markersize=15, label='Validation Predictions')\n",
        "        # red_line = mlines.Line2D([], [], color='red', markersize=15, label='Training Observations')\n",
        "        # magenta_line = mlines.Line2D([], [], color='magenta', markersize=15, label='Training Predictions')\n",
        "        plt.legend(handles=[orange_line, green_line])\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.title('All symbolic rates')\n",
        "        # plt.grid(True)\n",
        "        plt.show()\n",
        "\n",
        "        # full_test_predictions.to_csv(base_path + '/save_forecasts/reyes_lstm_forecasted.csv', index=False)"
      ],
      "id": "CCC1yTUe8jmK"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZbBhJ8YqCr4K"
      },
      "outputs": [],
      "source": [
        "LSTM_pipeline(reyes_sym_data.copy(),window_size=3)"
      ],
      "id": "ZbBhJ8YqCr4K"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwgADAML0acp"
      },
      "source": [
        "# Multivariate Forecasting"
      ],
      "id": "fwgADAML0acp"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5Qq8O0wvaEr"
      },
      "source": [
        "- VAR/VECM\n",
        "- Check stationarity of individual series in multivariate series\n",
        "- if all stationary, proceed with VAR.\n",
        "- if some/all non-stationary, check for cointegration\n",
        "- if no cointegration, convert to stationary as before and proceed with VAR.\n",
        "- convert data to stationary using logarithm and differencing.\n",
        "- revert predictions to the non-stationary scale using cumulative summation and exponential.\n",
        "- if cointegration, proceed with VECM."
      ],
      "id": "s5Qq8O0wvaEr"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-fSZgqp_9qK"
      },
      "source": [
        "### VAR/VECM"
      ],
      "id": "r-fSZgqp_9qK"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yJdjXI_ca7R_"
      },
      "outputs": [],
      "source": [
        "# temp = np.log(train.head(1))\n",
        "# new_df = np.log(train).diff().dropna()\n",
        "# new_df = pd.concat([temp, new_df])\n",
        "# np.exp(new_df.cumsum())"
      ],
      "id": "yJdjXI_ca7R_"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SBAFoBs9bg8z"
      },
      "outputs": [],
      "source": [
        "data = reyes_sym_data.copy()"
      ],
      "id": "SBAFoBs9bg8z"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c2YJwnTAc2nT"
      },
      "outputs": [],
      "source": [
        "train, test = traintestsplit(data, val_size=0, test_size=0.1)\n",
        "\n",
        "plt.plot(train.index, train)\n",
        "plt.plot(test.index, test)\n",
        "plt.legend(['Train', 'Test'])\n",
        "plt.ylabel('Close Price')\n",
        "plt.xlabel('Time')\n",
        "plt.title('Stock: SPY (S&P 500), Range: 1yr, Frequency: 1day')"
      ],
      "id": "c2YJwnTAc2nT"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pLnIARAmIriG"
      },
      "outputs": [],
      "source": [
        "train"
      ],
      "id": "pLnIARAmIriG"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Crz4ACuhNsHP"
      },
      "outputs": [],
      "source": [
        "# print(\"--CHECK STATIONARITY--\")\n",
        "# check_stationarity(train.close)\n",
        "# check_stationarity(train['min'])\n",
        "# check_stationarity(train['q25'])\n",
        "# check_stationarity(train['q50'])\n",
        "# check_stationarity(train['q75'])\n",
        "# check_stationarity(train['max'])"
      ],
      "id": "Crz4ACuhNsHP"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jz7vni7-SD7A"
      },
      "outputs": [],
      "source": [
        "# Y = train['q25']\n",
        "# X1 = train['q50']\n",
        "# X2 = train['q75']\n",
        "\n",
        "# # Combine the X series into a 2D array\n",
        "# X = np.column_stack((X1, X2))\n",
        "\n",
        "# # Add a constant to X\n",
        "# X = sm.add_constant(X)\n",
        "\n",
        "# # Estimate the model\n",
        "# model = sm.OLS(Y, X)\n",
        "# results = model.fit()\n",
        "\n",
        "# # Print the coefficients\n",
        "# print(results.params)\n",
        "# # Engle-Granger test: apply results.resid to ADF test\n",
        "# check_stationarity(results.resid)\n",
        "\n",
        "# Johansen test\n",
        "# Apply the Johansen cointegration test\n",
        "johansen_test = vecm.coint_johansen(train, det_order=0, k_ar_diff=1)\n",
        "\n",
        "# Print the eigenvalues\n",
        "print('Eigenvalues:\\n', johansen_test.lr1)\n",
        "\n",
        "# Print the trace statistics\n",
        "print('Trace Statistics:\\n', johansen_test.lr2)\n",
        "\n",
        "# Print the critical values\n",
        "print('Critical Values (90%, 95%, 99%):\\n', johansen_test.cvt)\n"
      ],
      "id": "jz7vni7-SD7A"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOxmOeY3h2oW"
      },
      "source": [
        "### VAR"
      ],
      "id": "VOxmOeY3h2oW"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P6n6VOfvv74a"
      },
      "outputs": [],
      "source": [
        "# temp = np.log(data).tail(1)\n",
        "# stationary_data = np.log(data).diff().dropna()\n",
        "# train, test = traintestsplit(stationary_data, val_size=0, test_size=0.1)\n",
        "\n",
        "# stationary_data, test_stationary_data = train[1:].copy(), test.copy()\n",
        "# temp = np.log(train).tail(1)\n",
        "# stationary_data['q25'] = np.log(train['q25']).diff().dropna()\n",
        "# temp_test = pd.concat([train['q25'].tail(1), test['q25']])\n",
        "# test_stationary_data['q25'] = np.log(temp_test).diff().dropna()\n",
        "# test_stationary_data\n",
        "\n",
        "stationary_data, test_stationary_data = train.copy(), test.copy()\n",
        "stationary_data['Q1'] = np.log(train['Q1'])\n",
        "test_stationary_data['Q1'] = np.log(test['Q1'])"
      ],
      "id": "P6n6VOfvv74a"
    },
    {
      "cell_type": "code",
      "source": [
        "# stationary_data, test_stationary_data = train[1:].copy(), test.copy()\n",
        "# stationary_data['Q1'] = np.log(train['Q1']).diff().dropna()\n",
        "# temp_test = np.log(pd.concat([train.tail(1), test])).diff().dropna()\n",
        "# test_stationary_data['Q1'] = temp_test['Q1']"
      ],
      "metadata": {
        "id": "WBGqGtBt3Qbl"
      },
      "id": "WBGqGtBt3Qbl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UEzJ6_S_0eVD"
      },
      "outputs": [],
      "source": [
        "plt.plot(stationary_data)\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ],
      "id": "UEzJ6_S_0eVD"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NF-u93Ua0ehb"
      },
      "outputs": [],
      "source": [
        "var = VAR(stationary_data)\n",
        "\n",
        "order = var.select_order()\n",
        "order.summary()"
      ],
      "id": "NF-u93Ua0ehb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHDBs3409bfF"
      },
      "outputs": [],
      "source": [
        "# Fit the VAR model, returns the Estimation results\n",
        "estimation_results = var.fit(7)\n",
        "# Compute output summary of estimates\n",
        "estimation_results.summary()"
      ],
      "id": "iHDBs3409bfF"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9iH3XBfC-Nvu"
      },
      "outputs": [],
      "source": [
        "# Fetch the lag order\n",
        "lag_order = estimation_results.k_ar\n",
        "print(lag_order)\n",
        "\n",
        "# in-sample predictions\n",
        "pred = estimation_results.fittedvalues\n",
        "pred = pd.DataFrame(pred, index=train.index)\n",
        "pred.columns = ['Q1', 'Q2', 'Q3']\n",
        "\n",
        "# out-of-sample forecasts\n",
        "forecasts = []\n",
        "history = stationary_data.copy()\n",
        "\n",
        "for t in range(0,len(test)-4,5):\n",
        "    # Forecast one step ahead\n",
        "    forecast = estimation_results.forecast(history.values[-lag_order:], 5)\n",
        "    # Append the forecast to your list of forecasts\n",
        "    forecasts.extend(forecast)\n",
        "    # Append the actual observation from test set to your data (to be used for the next forecast)\n",
        "    for i in range(5):\n",
        "        history = pd.concat([history, pd.DataFrame([test_stationary_data.iloc[t+i,:]])])\n",
        "\n",
        "forecasts = pd.DataFrame(forecasts, index=test.index[:len(forecasts)])\n",
        "forecasts.columns = ['Q1', 'Q2', 'Q3']\n",
        "\n",
        "forecasts['Q1'] = np.exp(forecasts['Q1'])\n",
        "# temp_forecast = (pd.concat([np.log(train['Q1'].tail(1)), forecasts['Q1']])).cumsum()\n",
        "# forecasts['Q1'] = np.exp(temp_forecast[1:])\n",
        "\n",
        "# pred_df = math_coherence(pred)\n",
        "forecast_df = math_coherence(forecasts)\n",
        "train_df = math_coherence(train)\n",
        "test_df = math_coherence(test)\n",
        "\n",
        "test_df['min'] = (test_df['Q1'] - (1.5*(test_df['Q3'] - test_df['Q1'])))\n",
        "test_df['max'] = (test_df['Q3'] + (1.5*(test_df['Q3'] - test_df['Q1'])))\n",
        "forecast_df['min'] = (forecast_df['Q1'] - (1.5*(forecast_df['Q3'] - forecast_df['Q1'])))\n",
        "forecast_df['max'] = (forecast_df['Q3'] + (1.5*(forecast_df['Q3'] - forecast_df['Q1'])))"
      ],
      "id": "9iH3XBfC-Nvu"
    },
    {
      "cell_type": "code",
      "source": [
        "test_df"
      ],
      "metadata": {
        "id": "J7ZSGjlzHQ5p"
      },
      "id": "J7ZSGjlzHQ5p",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "performance(test_df[:-4], forecast_df)\n",
        "# forecast_df.to_csv(base_path + '/save_forecasts/eurusd_var_forecasted.csv', index=False)"
      ],
      "metadata": {
        "id": "j--If8tOS6-i"
      },
      "id": "j--If8tOS6-i",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_test = test_df[:-4]"
      ],
      "metadata": {
        "id": "FKFq3y_vHukF"
      },
      "id": "FKFq3y_vHukF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "27PUSDjS_yYy"
      },
      "outputs": [],
      "source": [
        "for col in test_df.columns:\n",
        "    plt.plot(new_test.index, new_test[col], color='orange', label='Testing Observations')\n",
        "    plt.plot(forecast_df.index, forecast_df[col], color='green', label='Testing Predictions')\n",
        "    plt.xticks(rotation=45)\n",
        "    orange_line = mlines.Line2D([], [], color='orange', markersize=15, label='Testing Observations')\n",
        "    green_line = mlines.Line2D([], [], color='green', markersize=15, label='Testing Predictions')\n",
        "    plt.legend(handles=[orange_line, green_line])\n",
        "    plt.title(f'{col} rates')\n",
        "    plt.show()\n",
        "\n",
        "# plt.plot(train_df.index, train_df, color='lightblue', label='train')\n",
        "plt.plot(test_df.index, test_df, color='orange', label='Testing Observations')\n",
        "# plt.plot(pred_df.index, pred_df, color='peachpuff', label='in-sample prediction', lw=0.9)\n",
        "plt.plot(forecast_df.index, forecast_df, color='green', label='Testing Predictions', lw=0.9)\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "# Create a custom legend\n",
        "# lightblue_line = mlines.Line2D([], [], color='lightblue', markersize=15, label='train')\n",
        "orange_line = mlines.Line2D([], [], color='orange', markersize=15, label='Testing Observations')\n",
        "# peachpuff_line = mlines.Line2D([], [], color='peachpuff', markersize=15, label='in-sample prediction')\n",
        "green_line = mlines.Line2D([], [], color='green', markersize=15, label='Testing Predictions')\n",
        "plt.legend(handles=[orange_line, green_line])\n",
        "plt.title('All symbolic rates')\n",
        "plt.show()"
      ],
      "id": "27PUSDjS_yYy"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZvSV7MNiMfg"
      },
      "source": [
        "### VECM"
      ],
      "id": "pZvSV7MNiMfg"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N0yMvzSYo5zv"
      },
      "outputs": [],
      "source": [
        "model = var2(train)\n",
        "results = model.select_order()\n",
        "print(results.summary())"
      ],
      "id": "N0yMvzSYo5zv"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xWA2RHH_ukWU"
      },
      "outputs": [],
      "source": [
        "lag_order = 24\n",
        "# coint_rank determined from johansen test: no. of cointegrated time series -1\n",
        "model = vecm.VECM(train, k_ar_diff = lag_order, coint_rank = 2, deterministic=\"ci\")\n",
        "result = model.fit()\n",
        "# result.summary()\n",
        "\n",
        "# produce in-sample forecasts\n",
        "pred = result.fittedvalues\n",
        "pred = pd.DataFrame(pred, index=train.index[lag_order:-1])\n",
        "pred.columns = ['Q1', 'Q2', 'Q3']\n",
        "# out-of-sample forecasts\n",
        "forecasts = []\n",
        "history = train.copy()\n",
        "history = history.values.tolist()\n",
        "\n",
        "for t in range(len(test)):\n",
        "    # Forecast one step ahead\n",
        "    model = vecm.VECM(history, k_ar_diff = lag_order, coint_rank = 2, deterministic=\"ci\")\n",
        "    result = model.fit()\n",
        "    # Forecast one step ahead\n",
        "    forecast = result.predict(steps=1)\n",
        "    # Append the forecast to your list of forecasts\n",
        "    forecasts.append(forecast[0])\n",
        "    # Append the actual observation from test set to your data (to be used for the next forecast)\n",
        "    history.append(test.iloc[t,:])\n",
        "    # history = pd.concat([history, pd.DataFrame([test.iloc[t,:]])])\n",
        "\n",
        "# forecasts = result.predict(steps=len(test))\n",
        "\n",
        "forecasts = pd.DataFrame(forecasts, index=test.index)\n",
        "forecasts.columns = ['Q1', 'Q2', 'Q3']\n",
        "\n",
        "pred_df = math_coherence(pred)\n",
        "forecast_df = math_coherence(forecasts)\n",
        "train_df = math_coherence(train)\n",
        "test_df = math_coherence(test)\n",
        "\n",
        "test_df['min'] = (test_df['Q1'] - (1.5*(test_df['Q3'] - test_df['Q1'])))\n",
        "test_df['max'] = (test_df['Q3'] + (1.5*(test_df['Q3'] - test_df['Q1'])))\n",
        "forecast_df['min'] = (forecast_df['Q1'] - (1.5*(forecast_df['Q3'] - forecast_df['Q1'])))\n",
        "forecast_df['max'] = (forecast_df['Q3'] + (1.5*(forecast_df['Q3'] - forecast_df['Q1'])))"
      ],
      "id": "xWA2RHH_ukWU"
    },
    {
      "cell_type": "code",
      "source": [
        "training = train_df[lag_order:-1]\n",
        "predictions = pred_df.copy()\n",
        "training['min'] = (training['Q1'] - (1.5*(training['Q3'] - training['Q1'])))\n",
        "training['max'] = (training['Q3'] + (1.5*(training['Q3'] - training['Q1'])))\n",
        "predictions['min'] = (predictions['Q1'] - (1.5*(predictions['Q3'] - predictions['Q1'])))\n",
        "predictions['max'] = (predictions['Q3'] + (1.5*(predictions['Q3'] - predictions['Q1'])))\n",
        "\n",
        "performance(training, predictions)"
      ],
      "metadata": {
        "id": "GRro5vTZxPSM"
      },
      "id": "GRro5vTZxPSM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "performance(test_df, forecast_df)\n",
        "# forecast_df.to_csv(base_path + '/save_forecasts/btc_vecm_forecasted.csv', index=False)"
      ],
      "metadata": {
        "id": "aD2s3GmNTQaX"
      },
      "id": "aD2s3GmNTQaX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sc7OfaDhw6tK"
      },
      "outputs": [],
      "source": [
        "for col in test_df.columns:\n",
        "    plt.plot(test_df.index, test_df[col], color='orange', label='Testing Observations')\n",
        "    plt.plot(forecast_df.index, forecast_df[col], color='green', label='Testing Predictions')\n",
        "    plt.xticks(rotation=45)\n",
        "    orange_line = mlines.Line2D([], [], color='orange', markersize=15, label='Testing Observations')\n",
        "    green_line = mlines.Line2D([], [], color='green', markersize=15, label='Testing Predictions')\n",
        "    plt.legend(handles=[orange_line, green_line])\n",
        "    plt.title(f'{col} prices')\n",
        "    plt.show()\n",
        "\n",
        "# plt.plot(train_df.index, train_df, color='lightblue', label='train')\n",
        "plt.plot(test_df.index, test_df, color='orange', label='Testing Observations')\n",
        "# plt.plot(pred_df.index, pred_df, color='peachpuff', label='in-sample prediction', lw=0.9)\n",
        "plt.plot(forecast_df.index, forecast_df, color='green', label='Testing Predictions', lw=0.9)\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "# Create a custom legend\n",
        "# lightblue_line = mlines.Line2D([], [], color='lightblue', markersize=15, label='train')\n",
        "orange_line = mlines.Line2D([], [], color='orange', markersize=15, label='Testing Observations')\n",
        "# peachpuff_line = mlines.Line2D([], [], color='peachpuff', markersize=15, label='in-sample prediction')\n",
        "green_line = mlines.Line2D([], [], color='green', markersize=15, label='Testing Predictions')\n",
        "plt.legend(handles=[orange_line, green_line])\n",
        "plt.title('All symbolic prices')\n",
        "plt.show()"
      ],
      "id": "sc7OfaDhw6tK"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpad_qmAAEgr"
      },
      "source": [
        "### LSTM"
      ],
      "id": "kpad_qmAAEgr"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xsnlw8RMAHFw"
      },
      "outputs": [],
      "source": [
        "data = reyes_sym_data.copy()"
      ],
      "id": "xsnlw8RMAHFw"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GL4QQl82ghKY"
      },
      "outputs": [],
      "source": [
        "def df_to_windowed_df(df, window_size):\n",
        "    df = df.copy()\n",
        "    windowed_data = []\n",
        "    target_data = []\n",
        "    date_data = df.index\n",
        "    for i in range(window_size, len(df)):\n",
        "        window = df.iloc[i-window_size:i]\n",
        "        target = df.iloc[i]\n",
        "        windowed_data.append(window)\n",
        "        target_data.append(target)\n",
        "\n",
        "    windowed_data, target_data = np.array(windowed_data), np.array(target_data)\n",
        "\n",
        "    return date_data[window_size:], windowed_data, target_data"
      ],
      "id": "GL4QQl82ghKY"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2L6YThi3hwbC"
      },
      "outputs": [],
      "source": [
        "date, X, y = df_to_windowed_df(data, window_size=3)\n",
        "X = np.reshape(X,(X.shape[0], X.shape[1], X.shape[2]))\n",
        "print(X.shape)\n",
        "print(y.shape)\n",
        "print(date.shape)"
      ],
      "id": "2L6YThi3hwbC"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gWjH_EPqeyav"
      },
      "outputs": [],
      "source": [
        "dates_train, dates_val, dates_test = traintestsplit(date, val_size=0.1, test_size=0.1)\n",
        "X_train, X_val, X_test = traintestsplit(X, val_size=0.1, test_size=0.1)\n",
        "y_train, y_val, y_test = traintestsplit(y, val_size=0.1, test_size=0.1)\n",
        "\n",
        "# 0 = min variable... 4 = max variable\n",
        "for i in range(3):\n",
        "  plt.plot(dates_train, y_train[:,i])\n",
        "  plt.plot(dates_val, y_val[:,i])\n",
        "  plt.plot(dates_test, y_test[:,i])\n",
        "  plt.xticks(rotation=45)\n",
        "\n",
        "plt.legend(['Train', 'Validation', 'Test'])\n",
        "plt.ylabel('Close Price')\n",
        "plt.xlabel('Time')\n",
        "plt.title('Stock: SPY (S&P 500), Range: 1yr, Frequency: 1day')"
      ],
      "id": "gWjH_EPqeyav"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "61OjsnNr6lvW"
      },
      "outputs": [],
      "source": [
        "print(dates_train.shape)\n",
        "print(dates_val.shape)\n",
        "print(dates_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_val.shape)\n",
        "print(y_test.shape)\n",
        "print(X_train.shape)\n",
        "print(X_val.shape)\n",
        "print(X_test.shape)"
      ],
      "id": "61OjsnNr6lvW"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HD8YvLTpQtPW"
      },
      "outputs": [],
      "source": [
        "# Suppress TensorFlow logs\n",
        "logging.getLogger('tensorflow').setLevel(logging.ERROR)\n",
        "\n",
        "# Suppress Python warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def LSTM_tuning(X_train, y_train, X_val, y_val):\n",
        "    np.random.seed(20)\n",
        "    tf.random.set_seed(20)\n",
        "    # Clear TensorFlow session\n",
        "    K.clear_session()\n",
        "    # Function to create model\n",
        "    def create_model(learn_rate=0.001, neurons=32, dropout_rate=0, activation='relu'):\n",
        "        # input layers\n",
        "        inputs = Input(shape=(3,3))\n",
        "        x = inputs\n",
        "        x = LSTM(units=neurons, activation=activation, return_sequences=True)(x)\n",
        "        # middle layer(s)\n",
        "        x = Dropout(dropout_rate)(x)\n",
        "        x = LSTM(units=neurons, activation=activation, return_sequences=False)(x)\n",
        "        x = Dense(units=neurons, activation=activation)(x)\n",
        "        # last layer\n",
        "        outputs = Dense(3)(x)\n",
        "\n",
        "        model = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "        optimizer = Adam(learning_rate=learn_rate)\n",
        "        model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
        "\n",
        "        return model\n",
        "\n",
        "    # Wrap Keras model with KerasRegressor\n",
        "    model = KerasRegressor(build_fn=create_model, learn_rate=0.001, neurons=32, dropout_rate=0.0, activation='relu', verbose=0, epochs=50, batch_size=32)\n",
        "\n",
        "    # Define the parameter space\n",
        "    search_spaces = {\n",
        "        'learn_rate': (0.001, 0.01, 'log-uniform'),  # log-uniform distribution\n",
        "        'neurons': [32, 64, 128],  # list of choices\n",
        "        'dropout_rate': (0.0, 0.5, 'uniform'),  # uniform distribution\n",
        "        'batch_size': (32, 128, 'uniform'),  # uniform distribution\n",
        "    }\n",
        "    # Create TimeSeriesSplit object\n",
        "    tscv = TimeSeriesSplit(n_splits=3)\n",
        "\n",
        "    # Perform Bayesian Search Optimisation\n",
        "    bayes = BayesSearchCV(estimator=model, search_spaces=search_spaces, n_iter=10, cv=tscv, n_jobs=-1, random_state=20)\n",
        "    bayes_result = bayes.fit(X_train, y_train)\n",
        "\n",
        "    best_params = bayes_result.best_params_\n",
        "    # Build the model using best parameters\n",
        "    best_model = create_model(learn_rate=best_params['learn_rate'],\n",
        "                  neurons=best_params['neurons'],\n",
        "                  dropout_rate=best_params['dropout_rate'])\n",
        "\n",
        "    # Extract the best batch size from the best parameters\n",
        "    best_batch_size = best_params['batch_size']\n",
        "\n",
        "    # Define a ModelCheckpoint callback\n",
        "    cp = ModelCheckpoint('best_model_m', monitor='val_loss', verbose=0, save_best_only=True, mode='min')\n",
        "    # Fit the best model\n",
        "    best_model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=50, batch_size=best_params['batch_size'], callbacks=[cp], verbose=0)\n",
        "\n",
        "    # Load the best model\n",
        "    best_model.load_weights('best_model_m')\n",
        "\n",
        "    print(\"Best Hyperparameters:\")\n",
        "    for param, value in best_params.items():\n",
        "        print(f\"{param}: {value}\")\n",
        "\n",
        "    return best_model\n",
        "\n",
        "def iterative_forecasting(model, X, steps=5):\n",
        "    future_values = []\n",
        "    print(X.shape)\n",
        "    for i in range(0, len(X) - steps + 1, steps):\n",
        "        input_sequence = X[i:i+1, :, :].copy()  # Take one sample from X\n",
        "        for _ in range(steps):\n",
        "            next_value = model.predict(input_sequence, verbose=0)\n",
        "            future_values.append(next_value.flatten())\n",
        "            input_sequence = np.roll(input_sequence, shift=-1, axis=1)\n",
        "            input_sequence[0, -1, :] = next_value  # Assuming a univariate time series\n",
        "\n",
        "    return np.array(future_values).reshape(-1,3)"
      ],
      "id": "HD8YvLTpQtPW"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "maI77Or-DasZ"
      },
      "outputs": [],
      "source": [
        "model = LSTM_tuning(X_train, y_train, X_val, y_val)\n",
        "# train_predictions = model.predict(X_train)\n",
        "# train_predictions_df = math_coherence(train_predictions)\n",
        "# y_train_df = math_coherence(y_train)\n",
        "# plt.plot(dates_train, train_predictions_df, color='orange', label='Training Predictions')\n",
        "# plt.plot(dates_train, y_train_df, color='blue', label='Training Observations')\n",
        "# plt.xticks(rotation=45)\n",
        "# # Create a custom legend\n",
        "# blue_line = mlines.Line2D([], [], color='blue', markersize=15, label='Training Observations')\n",
        "# orange_line = mlines.Line2D([], [], color='orange', markersize=15, label='Training Predictions')\n",
        "# plt.legend(handles=[blue_line, orange_line])"
      ],
      "id": "maI77Or-DasZ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gVqd9CZiDhY3"
      },
      "outputs": [],
      "source": [
        "# val_predictions = model.predict(X_val)\n",
        "\n",
        "# val_predictions_df = math_coherence(val_predictions)\n",
        "# y_val_df = math_coherence(y_val)\n",
        "\n",
        "# plt.plot(dates_val, val_predictions_df, color='orange', label='Validation Predictions')\n",
        "# plt.plot(dates_val, y_val_df, color='blue', label='Validation Observations')\n",
        "# plt.xticks(rotation=45)\n",
        "# # Create a custom legend\n",
        "# blue_line = mlines.Line2D([], [], color='blue', markersize=15, label='Validation Observations')\n",
        "# orange_line = mlines.Line2D([], [], color='orange', markersize=15, label='Validation Predictions')\n",
        "# plt.legend(handles=[blue_line, orange_line])"
      ],
      "id": "gVqd9CZiDhY3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g8Dsh63yDmJq"
      },
      "outputs": [],
      "source": [
        "test_predictions = model.predict(X_test)\n",
        "test_predictions_df = math_coherence(test_predictions)\n",
        "\n",
        "y_test_df = math_coherence(y_test)\n",
        "\n",
        "y_test_df['min'] = (y_test_df['Q1'] - (1.5*(y_test_df['Q3'] - y_test_df['Q1'])))\n",
        "y_test_df['max'] = (y_test_df['Q3'] + (1.5*(y_test_df['Q3'] - y_test_df['Q1'])))\n",
        "test_predictions_df['min'] = (test_predictions_df['Q1'] - (1.5*(test_predictions_df['Q3'] - test_predictions_df['Q1'])))\n",
        "test_predictions_df['max'] = (test_predictions_df['Q3'] + (1.5*(test_predictions_df['Q3'] - test_predictions_df['Q1'])))\n",
        "\n",
        "performance(y_test_df, test_predictions_df)\n",
        "\n",
        "# test_predictions_df.to_csv(base_path + '/save_forecasts/eurusd_lstm_m_forecasted.csv', index=False)\n",
        "\n",
        "for col in y_test_df.columns:\n",
        "    plt.plot(dates_test, y_test_df[col], color='orange', label='Testing Observations')\n",
        "    plt.plot(dates_test, test_predictions_df[col], color='green', label='Testing Predictions')\n",
        "    plt.xticks(rotation=45)\n",
        "    orange_line = mlines.Line2D([], [], color='orange', markersize=15, label='Testing Observations')\n",
        "    green_line = mlines.Line2D([], [], color='green', markersize=15, label='Testing Predictions')\n",
        "    plt.legend(handles=[orange_line, green_line])\n",
        "    plt.title(f'{col} rates')\n",
        "    plt.show()\n",
        "\n",
        "plt.plot(dates_test, y_test_df, color='orange', label='Testing Observations')\n",
        "plt.plot(dates_test, test_predictions_df, color='green', label='Testing Predictions')\n",
        "plt.xticks(rotation=45)\n",
        "# Create a custom legend\n",
        "orange_line = mlines.Line2D([], [], color='orange', markersize=15, label='Testing Observations')\n",
        "green_line = mlines.Line2D([], [], color='green', markersize=15, label='Testing Predictions')\n",
        "plt.title('All symbolic rates')\n",
        "plt.legend(handles=[orange_line, green_line])"
      ],
      "id": "g8Dsh63yDmJq"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "isCUbGJfEE38"
      },
      "outputs": [],
      "source": [
        "# plt.plot(dates_train, train_predictions_df, color='magenta', label = 'Training Predictions')\n",
        "# plt.plot(dates_train, y_train_df, color='red', label='Training Observations')\n",
        "# plt.plot(dates_val, val_predictions_df, color='peachpuff', label='Validation Predictions')\n",
        "# plt.plot(dates_val, y_val_df, color='lightblue', label='Validation Observations')\n",
        "# plt.plot(dates_test, test_predictions_df, color='orange', label='Testing Predictions')\n",
        "# plt.plot(dates_test, y_test_df, color='blue', label='Testing Observations')\n",
        "\n",
        "# # Create a custom legend\n",
        "# blue_line = mlines.Line2D([], [], color='blue', markersize=15, label='Testing Observations')\n",
        "# orange_line = mlines.Line2D([], [], color='orange', markersize=15, label='Testing Predictions')\n",
        "# lightblue_line = mlines.Line2D([], [], color='lightblue', markersize=15, label='Validation Observations')\n",
        "# peachpuff_line = mlines.Line2D([], [], color='peachpuff', markersize=15, label='Validation Predictions')\n",
        "# red_line = mlines.Line2D([], [], color='red', markersize=15, label='Training Observations')\n",
        "# magenta_line = mlines.Line2D([], [], color='magenta', markersize=15, label='Training Predictions')\n",
        "# plt.legend(handles=[blue_line, orange_line, lightblue_line, peachpuff_line, red_line, magenta_line])"
      ],
      "id": "isCUbGJfEE38"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Buy/Sell Trading Simulation"
      ],
      "metadata": {
        "id": "09EG0H9-Uedx"
      },
      "id": "09EG0H9-Uedx"
    },
    {
      "cell_type": "code",
      "source": [
        "def sym_trading_simulation(observed, forecasted):\n",
        "    initial_capital = 10000\n",
        "    # Lists to store timestamps of buy, sell, and hold actions\n",
        "    buy_dates = []\n",
        "    sell_dates = []\n",
        "    hold_dates = []\n",
        "\n",
        "    # Initialize variables\n",
        "    capital = initial_capital\n",
        "    asset_held = 0  # Number of units of the asset held\n",
        "    num_buys = 0\n",
        "    num_sells = 0\n",
        "\n",
        "    # Iterate through the data values\n",
        "    for date in range(1, len(forecasted)):  # Start from 1 because we need previous timestep's data\n",
        "        # Buy conditions\n",
        "        # Strong conditions\n",
        "        if forecasted['min'].iloc[date] > observed['max'].iloc[date-1]:\n",
        "            # Strong Buy\n",
        "            asset_held += capital / observed['Q2'].iloc[date]\n",
        "            capital = 0\n",
        "            num_buys += 1\n",
        "            buy_dates.append(forecasted.index[date])\n",
        "        elif forecasted['max'].iloc[date] < observed['min'].iloc[date-1]:\n",
        "            # Strong Sell\n",
        "            capital += asset_held * observed['Q2'].iloc[date]\n",
        "            asset_held = 0\n",
        "            num_sells += 1\n",
        "            sell_dates.append(forecasted.index[date])\n",
        "        # Moderate-Strong conditions\n",
        "        elif forecasted['min'].iloc[date] > observed['Q3'].iloc[date-1] or forecasted['Q1'].iloc[date] > observed['max'].iloc[date-1]:\n",
        "            # Moderate-Strong Buy\n",
        "            asset_held += (0.75 * capital) / observed['Q2'].iloc[date]\n",
        "            capital *= 0.25\n",
        "            num_buys += 1\n",
        "            buy_dates.append(forecasted.index[date])\n",
        "        elif forecasted['max'].iloc[date] < observed['Q1'].iloc[date-1] or forecasted['Q3'].iloc[date] < observed['min'].iloc[date-1]:\n",
        "            # Moderate-Strong Sell\n",
        "            capital += (0.75 * asset_held) * observed['Q2'].iloc[date]\n",
        "            asset_held *= 0.25\n",
        "            num_sells += 1\n",
        "            sell_dates.append(forecasted.index[date])\n",
        "        # Weak-Moderate conditions\n",
        "        elif (forecasted['min'].iloc[date] > observed['Q2'].iloc[date-1] or\n",
        "              forecasted['Q1'].iloc[date] > observed['Q3'].iloc[date-1] or\n",
        "              forecasted['Q2'].iloc[date] > observed['max'].iloc[date-1]):\n",
        "            # Weak-Moderate Buy\n",
        "            asset_held += (0.50 * capital) / observed['Q2'].iloc[date]\n",
        "            capital *= 0.50\n",
        "            num_buys += 1\n",
        "            buy_dates.append(forecasted.index[date])\n",
        "        elif (forecasted['max'].iloc[date] < observed['Q2'].iloc[date-1] or\n",
        "              forecasted['Q3'].iloc[date] < observed['Q1'].iloc[date-1] or\n",
        "              forecasted['Q2'].iloc[date] < observed['min'].iloc[date-1]):\n",
        "            # Weak-Moderate Sell\n",
        "            capital += (0.50 * asset_held) * observed['Q2'].iloc[date]\n",
        "            asset_held *= 0.50\n",
        "            num_sells += 1\n",
        "            sell_dates.append(forecasted.index[date])\n",
        "        # Weak conditions\n",
        "        elif (forecasted['min'].iloc[date] > observed['Q1'].iloc[date-1] or\n",
        "              forecasted['Q1'].iloc[date] > observed['Q2'].iloc[date-1] or\n",
        "              forecasted['Q2'].iloc[date] > observed['Q3'].iloc[date-1] or\n",
        "              forecasted['Q3'].iloc[date] > observed['max'].iloc[date-1]):\n",
        "            # Weak Buy\n",
        "            asset_held += (0.25 * capital) / observed['Q2'].iloc[date]\n",
        "            capital *= 0.75\n",
        "            num_buys += 1\n",
        "            buy_dates.append(forecasted.index[date])\n",
        "        elif (forecasted['max'].iloc[date] < observed['Q3'].iloc[date-1] or\n",
        "              forecasted['Q3'].iloc[date] < observed['Q2'].iloc[date-1] or\n",
        "              forecasted['Q2'].iloc[date] < observed['Q1'].iloc[date-1] or\n",
        "              forecasted['Q1'].iloc[date] < observed['min'].iloc[date-1]):\n",
        "            # Weak Sell\n",
        "            capital += (0.25 * asset_held) * observed['Q2'].iloc[date]\n",
        "            asset_held *= 0.75\n",
        "            num_sells += 1\n",
        "            sell_dates.append(forecasted.index[date])\n",
        "        else:\n",
        "            # Hold\n",
        "            hold_dates.append(forecasted.index[date])\n",
        "    # If there's any asset left at the end, sell it\n",
        "    capital += asset_held * observed['Q2'].iloc[-1]\n",
        "\n",
        "    # Calculate final capital and profit or loss\n",
        "    final_capital = capital\n",
        "    profit_or_loss = final_capital - initial_capital\n",
        "\n",
        "    # Plotting\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    observed['Q2'].plot(label='Observed Median (Q2)', color='blue')\n",
        "    forecasted['Q2'].plot(label='Forecasted Median (Q2)', color='orange')\n",
        "    plt.fill_between(observed.index, observed['min'], observed['max'], color='cyan', alpha=0.3, label='Observed Min-Max range')\n",
        "    plt.fill_between(forecasted.index, forecasted['min'], forecasted['max'], color='yellow', alpha=0.3, label='Forecasted Min-Max range')\n",
        "    plt.scatter(buy_dates, forecasted['Q2'][buy_dates], marker='^', color='g', label='Buy Signal', alpha=1)\n",
        "    plt.scatter(sell_dates, forecasted['Q2'][sell_dates], marker='v', color='r', label='Sell Signal', alpha=1)\n",
        "    plt.scatter(hold_dates, forecasted['Q2'][hold_dates], marker='o', color='gray', label='Hold Signal', alpha=0.5, s=50)\n",
        "    plt.legend()\n",
        "    plt.title('Boxplot Aggregated Time Series Trading Simulation')\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel('Value')\n",
        "    plt.show()\n",
        "\n",
        "    return final_capital, profit_or_loss, num_buys, num_sells, buy_dates, sell_dates, hold_dates\n",
        "\n",
        "def sp_trading_simulation(observed, forecasted):\n",
        "    observed = pd.Series(observed)\n",
        "    forecasted = pd.Series(forecasted)\n",
        "\n",
        "    # Initialize variables\n",
        "    initial_capital = 10000\n",
        "    capital = initial_capital\n",
        "    asset_held = 0  # Number of units of the asset held\n",
        "    num_buys = 0\n",
        "    num_sells = 0\n",
        "\n",
        "    # Lists to store timestamps of buy, sell, and hold actions\n",
        "    buy_dates = []\n",
        "    sell_dates = []\n",
        "    hold_dates = []\n",
        "\n",
        "    # Iterate through the observed and forecasted values\n",
        "    for date in range(1, len(forecasted)):\n",
        "        if forecasted.iloc[date] > 1.01 * observed.iloc[date-1]:  # Buy signal\n",
        "            # Buy as much as possible\n",
        "            asset_held += capital / observed.iloc[date]\n",
        "            capital = 0\n",
        "            num_buys += 1\n",
        "            buy_dates.append(observed.index[date])\n",
        "        elif forecasted.iloc[date] < 0.99 * observed.iloc[date-1]:  # Sell signal\n",
        "            # Sell all assets\n",
        "            capital += asset_held * observed.iloc[date]\n",
        "            asset_held = 0\n",
        "            num_sells += 1\n",
        "            sell_dates.append(observed.index[date])\n",
        "        else:  # Hold\n",
        "            hold_dates.append(observed.index[date])\n",
        "\n",
        "    # If there's any asset left at the end, sell it\n",
        "    capital += asset_held * observed.iloc[-1]\n",
        "\n",
        "    # Calculate final capital and profit or loss\n",
        "    final_capital = capital\n",
        "    profit_or_loss = final_capital - initial_capital\n",
        "\n",
        "    # Plotting\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    observed.plot(label='Observed', color='blue')\n",
        "    forecasted.plot(label='Forecasted', color='orange')\n",
        "    plt.scatter(buy_dates, forecasted[buy_dates], marker='^', color='g', label='Buy Signal', alpha=1)\n",
        "    plt.scatter(sell_dates, forecasted[sell_dates], marker='v', color='r', label='Sell Signal', alpha=1)\n",
        "    plt.scatter(hold_dates, forecasted[hold_dates], marker='o', color='gray', label='Hold Signal', alpha=0.5, s=50)\n",
        "    plt.legend()\n",
        "    plt.title('Mean-Aggregated Time Series Trading Simulation')\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel('Value')\n",
        "    plt.show()\n",
        "\n",
        "    return final_capital, profit_or_loss, num_buys, num_sells, buy_dates, sell_dates, hold_dates\n",
        "\n",
        "def baseline(observed, forecasted):\n",
        "    # Initial capital\n",
        "    capital = 10000\n",
        "\n",
        "    # Buy on the first date using all capital at Q2 price\n",
        "    asset_held = capital / observed['Q2'].iloc[0]\n",
        "    capital = 0  # Set capital to zero after the buy\n",
        "\n",
        "    # Sell on the last date using all assets at Q2_end price\n",
        "    Q2_end = observed['Q2'].iloc[-1]\n",
        "    capital = asset_held * Q2_end\n",
        "    asset_held = 0\n",
        "\n",
        "    # Calculate profit or loss\n",
        "    profit_or_loss = capital - 10000\n",
        "\n",
        "    return capital, profit_or_loss\n",
        "\n",
        "def baseline_sp(observed, forecasted):\n",
        "    # Initial capital\n",
        "    capital = 10000\n",
        "\n",
        "    # Buy on the first date using all capital at Q2 price\n",
        "    asset_held = capital / observed.iloc[0]\n",
        "    capital = 0  # Set capital to zero after the buy\n",
        "\n",
        "    # Sell on the last date using all assets at Q2_end price\n",
        "    close_end = observed.iloc[-1]\n",
        "    capital = asset_held * close_end\n",
        "    asset_held = 0\n",
        "\n",
        "    # Calculate profit or loss\n",
        "    profit_or_loss = capital - 10000\n",
        "\n",
        "    return capital, profit_or_loss"
      ],
      "metadata": {
        "id": "Bsw1FVT9UkYB"
      },
      "id": "Bsw1FVT9UkYB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "forecast = pd.read_csv(base_path + '/save_forecasts/btc_arima_ms_forecasted.csv')\n",
        "data = BTC_sym_data\n",
        "train, test_df = traintestsplit(data, val_size=0, test_size=0.1)\n",
        "test = math_coherence(test_df)\n",
        "\n",
        "test['min'] = (test['Q1'] - (1.5*(test['Q3'] - test['Q1'])))\n",
        "test['max'] = (test['Q3'] + (1.5*(test['Q3'] - test['Q1'])))\n",
        "# forecast['min'] = (forecast['Q1'] - (1.5*(forecast['Q3'] - forecast['Q1'])))\n",
        "# forecast['max'] = (forecast['Q3'] + (1.5*(forecast['Q3'] - forecast['Q1'])))\n",
        "forecast.index = test.index\n",
        "forecast.rename(columns={'q25':'Q1', 'q50':'Q2','q75':'Q3'}, inplace=True)\n",
        "print(test)\n",
        "print(forecast)"
      ],
      "metadata": {
        "id": "5IAJ7lLaSvGg"
      },
      "id": "5IAJ7lLaSvGg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "forecast_sp = pd.read_csv(base_path + '/save_forecasts/btc_arima_sp_ms_forecasted.csv')\n",
        "data_sp = BTC_sp_data\n",
        "train_sp, test_sp = traintestsplit(data_sp, val_size=0, test_size=0.1)\n",
        "forecast_sp.index = test_sp.index\n",
        "print(test_sp)\n",
        "print(forecast_sp)"
      ],
      "metadata": {
        "id": "ZSoys8CtYdFF"
      },
      "id": "ZSoys8CtYdFF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_capital, pnl = baseline(test, forecast)\n",
        "print(f\"Final Capital: ${final_capital:.2f}\")\n",
        "print(f\"Profit/Loss: ${pnl:.2f}\")\n",
        "\n",
        "final_capital_sp, pnl_sp = baseline_sp(test_sp, forecast_sp)\n",
        "print(final_capital_sp)\n",
        "print(pnl_sp)"
      ],
      "metadata": {
        "id": "p7zvptkDCADN"
      },
      "id": "p7zvptkDCADN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sp_trading_simulation(test_sp['close'], forecast_sp['close'])"
      ],
      "metadata": {
        "id": "8k_RNfSvWonZ"
      },
      "id": "8k_RNfSvWonZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sym_trading_simulation(test, forecast)"
      ],
      "metadata": {
        "id": "34xEDy6fWmze"
      },
      "id": "34xEDy6fWmze",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "de26d10b",
        "f5a61e41",
        "Gdq4XbTkRcqk",
        "3e91e31a",
        "aj8XlrkSm_o7",
        "fuz4HDqx8UVg",
        "qEH9TNkwOP0b",
        "786c00HzuQQC",
        "66fa763a",
        "5f6eb842",
        "ea3e6aa1",
        "7QBQ0vCKiV4D",
        "8_jZ8AaXdAgr",
        "pOXc-UiZbpBj",
        "iGPC9pFIwLty",
        "kFPgkRFmvVa1",
        "wkjal2nAbsP5",
        "vho43ntpIiCN",
        "D9CExOSc28Dr",
        "7xC6LP-rsfyD",
        "7z5z2endaqbH",
        "r-fSZgqp_9qK",
        "VOxmOeY3h2oW",
        "pZvSV7MNiMfg",
        "kpad_qmAAEgr"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}